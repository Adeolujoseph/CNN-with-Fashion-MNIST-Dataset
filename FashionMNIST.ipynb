{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTMKmCMYvOihwCWOtTtNkn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U8HL8Aul5BdE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Load the  dataset without normalization\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
        "\n",
        "# Calculate mean and std across the dataset\n",
        "data = train_dataset.data/ 255.  # Scale pixel values to [0, 1]\n",
        "mean = torch.mean(data)\n",
        "std= torch.std(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std Deviation:\", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7hyAABP5Ios",
        "outputId": "3b67b09e-997c-4b4a-9f9a-f66f5f3be7ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 18878422.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 319433.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5494158.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 14020959.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Mean: tensor(0.2860)\n",
            "Std Deviation: tensor(0.3530)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the MNIST dataset without normalization\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Calculate mean and std across the dataset\n",
        "data = test_dataset.data/ 255.  # Scale pixel values to [0, 1]\n",
        "mean = torch.mean(data)\n",
        "std= torch.std(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std Deviation:\", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWtHHGqB5_VG",
        "outputId": "e03ffcce-4ab8-4223-c312-cf21e93b07e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor(0.2868)\n",
            "Std Deviation: tensor(0.3524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
        "                                    transforms.Normalize(0.2860, 0.3530)])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
        "                                    transforms.Normalize(0.2868, 0.3524)])"
      ],
      "metadata": {
        "id": "rb4Z_yBK6Kyu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transforms)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=val_transforms)"
      ],
      "metadata": {
        "id": "Zg38RSJJ6tNP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, )\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, )\n",
        "print(f\"There are {len(train_dataset)} train images and {len(test_dataset)} val images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsNEGCLA67OL",
        "outputId": "df2c73e1-6e72-4992-c260-af66e3331368"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 60000 train images and 10000 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural net class\n",
        "class Net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Our images are grayscale, so input channels = 1. We'll apply 48 filters in the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=48, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        # We'll apply max pooling with a kernel size of 2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A second convolutional layer takes 48 input channels, and generates 96 outputs\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        # We'll apply another max pooling with a kernel size of 2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A drop layer deletes 30% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.3)\n",
        "\n",
        "        # Our 28x28 image tensors will be pooled twice with a kernel size of 2. 28/2/2 is 7.\n",
        "        # So our feature tensors are now 7 x 7, and we've generated 96 of them\n",
        "        # We need to flatten these and feed them to a fully-connected layer\n",
        "        # to map them to  the probability for each class\n",
        "        self.fc = nn.Linear(in_features=7 * 7 * 96, out_features=164)\n",
        "        self.fc2 = nn.Linear(in_features=164, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use a relu activation function after layer 1 (convolution 1 and pool)\n",
        "        x = F.relu(self.pool1(self.conv1(x)))\n",
        "\n",
        "        # Use a relu activation function after layer 2 (convolution 2 and pool)\n",
        "        x = F.relu(self.pool2(self.conv2(x)))\n",
        "\n",
        "        # Select some features to drop after the 2nd convolution to prevent overfitting\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 7 * 7 * 96)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        x = self.fc2(x)\n",
        "        # Return log_softmax tensor\n",
        "        return F.log_softmax(x, dim=1)\n",
        "# Instantiate the model\n",
        "model = Net()\n",
        "print(\"CNN model class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4IYY-i07EXT",
        "outputId": "d2d6c27e-db8d-408f-e110-9b4ccb8998cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Training set:{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)} Loss:{loss.item()})')\n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "XqrzvRBs7MaB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print(f'Validation set: Average loss: {avg_loss}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)')\n",
        "   # return average loss for the epoch\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "J9VYeTIH7RFw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = Net(num_classes=10).to(device)\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over  epochs\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyeaMhiJ7OC2",
        "outputId": "9e439bfd-a24c-484c-b9b3-6f9a2a747a92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Epoch: 1\n",
            "Training set:0/60000 (0.0 Loss:2.3154993057250977)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.8348628878593445)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:1.1482969522476196)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.8621668815612793)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.5984448790550232)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.6241539120674133)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.6839302778244019)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.6147406697273254)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.42751458287239075)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.40724924206733704)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.8106647729873657)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.5245866775512695)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.5297848582267761)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.3797703981399536)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.5747100114822388)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.5058166980743408)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.6353360414505005)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.606480598449707)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.28962135314941406)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.4420067369937897)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.47274866700172424)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.7561061978340149)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.42448103427886963)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.46720150113105774)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.7612979412078857)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.4295641779899597)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.31227871775627136)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.5094407200813293)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.6685693264007568)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.45360612869262695)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.4542946517467499)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.45083311200141907)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.38931193947792053)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.7762282490730286)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.4262297451496124)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.4830619692802429)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.40076571702957153)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.38256001472473145)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.3408258855342865)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.4745544493198395)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.37355002760887146)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.3056586980819702)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.41896069049835205)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.17841705679893494)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.49426406621932983)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.6257766485214233)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.3318832516670227)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.39301982522010803)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.33997267484664917)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.22827023267745972)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.32691675424575806)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.34194454550743103)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.6177274584770203)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.3786729872226715)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.24736036360263824)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.5224371552467346)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.4139561057090759)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.49932438135147095)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.3212342858314514)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.3681676387786865)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.36673301458358765)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.5615520477294922)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.5126246809959412)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.32512688636779785)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.16311970353126526)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.2901131212711334)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.2038325071334839)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.2202931046485901)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.22145512700080872)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.2792617678642273)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.5402471423149109)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.18652981519699097)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.47084879875183105)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.5275112390518188)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.20042367279529572)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.4099583029747009)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.4547160267829895)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.5913373231887817)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.41495272517204285)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.3665332496166229)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.2627679407596588)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.2660619616508484)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.3566555976867676)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.3076122999191284)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.296053409576416)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.4044339656829834)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.31850606203079224)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.36359459161758423)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.18049146234989166)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.37067875266075134)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.2209179550409317)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.31269434094429016)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.39590880274772644)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.20662736892700195)\n",
            "Training set: Average loss: 0.439376\n",
            "Validation set: Average loss: 0.3701582642117883, Accuracy: 8637/10000 (86.37%)\n",
            "Epoch: 2\n",
            "Training set:0/60000 (0.0 Loss:0.4559429883956909)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.24027550220489502)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.2822644114494324)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.30353567004203796)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.20877888798713684)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.20054787397384644)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.4403058886528015)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.21572500467300415)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.429470032453537)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.4228276312351227)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.33065053820610046)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.4560946226119995)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.25697463750839233)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.3421042263507843)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.1663295179605484)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.26610422134399414)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.24532142281532288)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.4646078944206238)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.4078201353549957)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.390543133020401)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.360470712184906)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.4837007224559784)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.36086541414260864)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.3854466676712036)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.3897750973701477)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.4467180669307709)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.40237388014793396)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.3811885714530945)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.4144442081451416)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.38598760962486267)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.3623424768447876)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.26266419887542725)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.22257238626480103)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.37783950567245483)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.2611507773399353)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.3452884256839752)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.2601917088031769)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.35776013135910034)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.15088780224323273)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.4014441668987274)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.27192357182502747)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.3114173114299774)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.298054039478302)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.2753646671772003)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.2935236394405365)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.35941651463508606)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.31816452741622925)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.34210115671157837)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.16269037127494812)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.16483482718467712)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.2746984660625458)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.2959221601486206)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.33550524711608887)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.3608305752277374)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.24201178550720215)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.2594386041164398)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.2914080321788788)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.49071234464645386)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.4351772665977478)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.44819819927215576)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.4025134742259979)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.2550169825553894)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.3578610122203827)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.3011074364185333)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.45036065578460693)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.2484123408794403)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.3141041696071625)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.15998192131519318)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.12263164669275284)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.26372337341308594)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.17651934921741486)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.37595728039741516)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.39171162247657776)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.49863699078559875)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.3918168246746063)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.28820523619651794)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.5522201657295227)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.4058918356895447)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.277246356010437)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.29512470960617065)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.46430978178977966)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.3131473958492279)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.33914509415626526)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.23231521248817444)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.3722640573978424)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.3605271577835083)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.30012497305870056)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.2211419939994812)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.5095123648643494)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.3173629939556122)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.14766770601272583)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.3074009120464325)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.3389730155467987)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.22899527847766876)\n",
            "Training set: Average loss: 0.318284\n",
            "Validation set: Average loss: 0.30608811148792314, Accuracy: 8869/10000 (88.69%)\n",
            "Epoch: 3\n",
            "Training set:0/60000 (0.0 Loss:0.17203378677368164)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.45566505193710327)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.10154180228710175)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.12562993168830872)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.40600794553756714)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.36320731043815613)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.3485947251319885)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.2293199747800827)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.291787326335907)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.2165662795305252)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.38148897886276245)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.22763702273368835)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.2741309106349945)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.39797088503837585)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.2812635600566864)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.2468951940536499)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.2306792438030243)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.4561331868171692)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.16549602150917053)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.31917089223861694)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.328188419342041)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.30475613474845886)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.21434125304222107)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.15671467781066895)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.20852473378181458)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.43365588784217834)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.2428293079137802)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.3237496316432953)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.2518526315689087)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.39532020688056946)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.288703054189682)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.265324205160141)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.3454640507698059)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.21015813946723938)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.37256649136543274)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.5349093079566956)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.21622608602046967)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.22913327813148499)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.3730776607990265)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.2459174245595932)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.1990658938884735)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.3536339998245239)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.15203920006752014)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.1581917107105255)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.3684020936489105)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.36006155610084534)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.34532955288887024)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.28496113419532776)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.32776308059692383)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.3540097177028656)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.47245168685913086)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.2635568380355835)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.2704310417175293)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.28460484743118286)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.1721305102109909)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.2707917392253876)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.1721845418214798)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.2778410017490387)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.3248864710330963)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.20919717848300934)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.2478877753019333)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.21423396468162537)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.12054190039634705)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.22377675771713257)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.2062380611896515)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.32412657141685486)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.43448084592819214)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.2731674611568451)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.33394837379455566)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.2525850832462311)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.2688586115837097)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.1960679441690445)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.31166085600852966)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.321682870388031)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.18843206763267517)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.13133132457733154)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.19369599223136902)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.28582578897476196)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.08668725192546844)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.2115091234445572)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.25248587131500244)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.31536543369293213)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.27705979347229004)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.25157541036605835)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.206352099776268)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.29040223360061646)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.2209303230047226)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.3144450783729553)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.22210882604122162)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.2732256054878235)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.3528440594673157)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.19033408164978027)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.2928602993488312)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.2753017842769623)\n",
            "Training set: Average loss: 0.281058\n",
            "Validation set: Average loss: 0.30069117271786283, Accuracy: 8922/10000 (89.22%)\n",
            "Epoch: 4\n",
            "Training set:0/60000 (0.0 Loss:0.14247751235961914)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.43581873178482056)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.36235541105270386)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.12372346222400665)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.2866998612880707)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.2600262761116028)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.2868235111236572)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.11120893806219101)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.26285499334335327)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.28388720750808716)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.22431573271751404)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.28640511631965637)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.23602019250392914)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.2905156910419464)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.35126712918281555)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.2277059406042099)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.35179728269577026)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.2469279170036316)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.1520315557718277)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.15084761381149292)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.4243333041667938)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.3711234927177429)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.2080327719449997)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.3048144578933716)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.2025451511144638)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.33227500319480896)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.17886808514595032)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.21799308061599731)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.354597806930542)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.22875478863716125)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.3733116388320923)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.3038583993911743)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.3307139575481415)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.17190714180469513)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.20616547763347626)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.1651863157749176)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.2239987999200821)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.2972736358642578)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.2432098388671875)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.4497043490409851)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.33469825983047485)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.2860468327999115)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.3426501154899597)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.28523507714271545)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.45550885796546936)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.2839716672897339)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.10317067801952362)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.23072724044322968)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.17454242706298828)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.3140413761138916)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.23622803390026093)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.258372038602829)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.0841149240732193)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.1910993754863739)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.2225293070077896)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.24589504301548004)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.24584577977657318)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.3092075288295746)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.28488901257514954)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.2334294617176056)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.23099569976329803)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.12099602073431015)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.2528788447380066)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.17680777609348297)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.29017600417137146)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.2144184112548828)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.240483820438385)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.4397587776184082)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.46950697898864746)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.2099829912185669)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.22682586312294006)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.09817396849393845)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.28236207365989685)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.5807763338088989)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.23382169008255005)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.14828568696975708)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.3118818402290344)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.2565940320491791)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.3006717562675476)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.4471927881240845)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.23368589580059052)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.11786794662475586)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.21900597214698792)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.2509230971336365)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.23317548632621765)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.2585158050060272)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.11094538122415543)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.1888434737920761)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.23949642479419708)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.3469487428665161)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.19291600584983826)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.1328916698694229)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.2685961425304413)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.15225502848625183)\n",
            "Training set: Average loss: 0.258072\n",
            "Validation set: Average loss: 0.2778516055387297, Accuracy: 8960/10000 (89.6%)\n",
            "Epoch: 5\n",
            "Training set:0/60000 (0.0 Loss:0.30650126934051514)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.3372814357280731)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.2210930436849594)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.23731885850429535)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.14110401272773743)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.2781911790370941)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.23652221262454987)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.12473207712173462)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.160779669880867)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.17503216862678528)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.22511759400367737)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.18775084614753723)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.11604677140712738)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.2582935392856598)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.1829107254743576)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.17183846235275269)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.21074381470680237)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.24689488112926483)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.19479615986347198)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.28047841787338257)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.27134647965431213)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.2776222825050354)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.13310426473617554)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.229546919465065)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.12468837201595306)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.15855807065963745)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.23688319325447083)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.27603983879089355)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.26516416668891907)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.20455682277679443)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.15653328597545624)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.3645075261592865)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.18488293886184692)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.22752118110656738)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.22456388175487518)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.15892302989959717)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.25014033913612366)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.3711414039134979)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.25601083040237427)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.2792667746543884)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.3199584484100342)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.3123995065689087)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.2842733561992645)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.1927533596754074)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.3632219731807709)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.19160035252571106)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.2814711630344391)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.2562766671180725)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.2394263744354248)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.17378605902194977)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.13234156370162964)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.1946125328540802)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.22936144471168518)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.44096359610557556)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.31775784492492676)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.21230366826057434)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.2909086048603058)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.19643671810626984)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.22638224065303802)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.13502167165279388)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.21517813205718994)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.18134315311908722)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.3601081371307373)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.43049511313438416)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.18143177032470703)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.2514425814151764)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.1236199289560318)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.20432567596435547)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.19658391177654266)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.25908923149108887)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.49652522802352905)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.2401273101568222)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.1597905308008194)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.21084089577198029)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.24600674211978912)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.27770745754241943)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.338477224111557)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.35807740688323975)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.2368755340576172)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.1038312166929245)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.3215300738811493)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.1850670725107193)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.26085540652275085)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.28683924674987793)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.2746480405330658)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.20569996535778046)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.22911038994789124)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.22809067368507385)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.23935817182064056)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.24841535091400146)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.3014887273311615)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.17576634883880615)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.2705025374889374)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.3113398551940918)\n",
            "Training set: Average loss: 0.241331\n",
            "Validation set: Average loss: 0.27023553440145626, Accuracy: 9027/10000 (90.27%)\n",
            "Epoch: 6\n",
            "Training set:0/60000 (0.0 Loss:0.21612663567066193)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.20709557831287384)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.12527582049369812)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.2679049074649811)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.35367828607559204)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.17065633833408356)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.12324424833059311)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.22677041590213776)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.25315311551094055)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.32793813943862915)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.14428754150867462)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.12004706263542175)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.23266270756721497)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.09078166633844376)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.25564154982566833)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.12505242228507996)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.2754753828048706)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.13707253336906433)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.18596623837947845)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.14901597797870636)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.2742847800254822)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.2281094342470169)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.3633064031600952)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.3243142068386078)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.20515529811382294)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.2880222201347351)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.2529573440551758)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.17849838733673096)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.1266503781080246)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.1519806683063507)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.22013042867183685)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.18282024562358856)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.242295041680336)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.26477745175361633)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.14968056976795197)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.23003795742988586)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.24612334370613098)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.22027668356895447)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.4244559109210968)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.20202192664146423)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.4345542788505554)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.40277624130249023)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.3741624653339386)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.13256807625293732)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.1725064516067505)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.24259953200817108)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.13096335530281067)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.2337803691625595)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.18844841420650482)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.15858404338359833)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.21661780774593353)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.11647842079401016)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.20495352149009705)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.24193857610225677)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.192999005317688)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.11902274191379547)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.1226317435503006)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.29304853081703186)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.31157052516937256)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.38689956068992615)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.3815331757068634)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.2131481170654297)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.20140394568443298)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.21109117567539215)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.2633642256259918)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.14277216792106628)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.19065487384796143)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.33925580978393555)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.2302107810974121)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.085475854575634)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.5418493151664734)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.19360332190990448)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.21211998164653778)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.2812390625476837)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.1589968055486679)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.3020738661289215)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.26097068190574646)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.2517836391925812)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.10316107422113419)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.3427651822566986)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.15866394340991974)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.24817126989364624)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.2793196141719818)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.18746137619018555)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.3587585687637329)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.14047375321388245)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.20783279836177826)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.2140389382839203)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.22343330085277557)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.18054506182670593)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.2902805507183075)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.1443638950586319)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.3628022074699402)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.2890530228614807)\n",
            "Training set: Average loss: 0.231691\n",
            "Validation set: Average loss: 0.25104378130595395, Accuracy: 9112/10000 (91.12%)\n",
            "Epoch: 7\n",
            "Training set:0/60000 (0.0 Loss:0.21206776797771454)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.3390042185783386)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.2404485046863556)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.2289770543575287)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.15829038619995117)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.1909738928079605)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.14795342087745667)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.21641317009925842)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.19613617658615112)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.22330890595912933)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.19222646951675415)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.28575751185417175)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.25296029448509216)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.1899339109659195)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.3557192385196686)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.2422921061515808)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.2473214566707611)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.16816070675849915)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.2524471879005432)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.2158471941947937)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.22517287731170654)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.21740496158599854)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.22933447360992432)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.21467752754688263)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.22282563149929047)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.2296757996082306)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.1857638657093048)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.11378033459186554)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.3507557809352875)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.19737662374973297)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.18837684392929077)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.24421575665473938)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.19739940762519836)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.2132640928030014)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.18551097810268402)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.2259306013584137)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.24579128623008728)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.32544592022895813)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.1976347416639328)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.1422101855278015)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.21179524064064026)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.08614589273929596)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.1755210906267166)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.2217545211315155)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.24588192999362946)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.13778872787952423)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.12051297724246979)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.20314162969589233)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.33804410696029663)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.16677053272724152)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.19194361567497253)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.18487553298473358)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.17949874699115753)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.24973605573177338)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.2795209586620331)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.08959636837244034)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.1841270923614502)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.23273752629756927)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.06003028526902199)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.1885930895805359)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.20541875064373016)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.4627951979637146)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.24789226055145264)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.20264534652233124)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.23715166747570038)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.2923036217689514)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.3060435950756073)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.3590002954006195)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.23616603016853333)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.16260720789432526)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.2678816616535187)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.12561021745204926)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.28518250584602356)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.1690237820148468)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.2502535581588745)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.2850320637226105)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.39145228266716003)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.4656349718570709)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.29617899656295776)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.26054081320762634)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.1401711106300354)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.25804269313812256)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.21794246137142181)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.10541600733995438)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.18988129496574402)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.316499263048172)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.13714073598384857)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.17549598217010498)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.3552371561527252)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.2327759712934494)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.163951113820076)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.22340792417526245)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.25875112414360046)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.2185879349708557)\n",
            "Training set: Average loss: 0.219856\n",
            "Validation set: Average loss: 0.239370415924461, Accuracy: 9135/10000 (91.35%)\n",
            "Epoch: 8\n",
            "Training set:0/60000 (0.0 Loss:0.061510778963565826)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.12499386072158813)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.13571728765964508)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.19466154277324677)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.16706256568431854)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.1508108228445053)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.25224846601486206)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.1735578030347824)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.20358413457870483)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.23020297288894653)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.2074013650417328)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.11456608772277832)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.09880953282117844)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.11934342980384827)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.09135995805263519)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.26893720030784607)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.3759434223175049)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.1628323793411255)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.21695540845394135)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.3914274275302887)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.2763836979866028)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.3073345720767975)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.2438698709011078)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.19764308631420135)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.43034419417381287)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.2818892300128937)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.18838512897491455)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.2975105345249176)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.27858129143714905)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.12544378638267517)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.15055541694164276)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.11439120024442673)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.16042666137218475)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.11494524031877518)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.22111517190933228)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.18035046756267548)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.24915798008441925)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.23878364264965057)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.13141195476055145)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.22349607944488525)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.09682133048772812)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.10051175951957703)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.15302658081054688)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.2153102159500122)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.19937138259410858)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.3151058256626129)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.16430431604385376)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.140090212225914)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.12511733174324036)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.35808295011520386)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.23345689475536346)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.16770708560943604)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.05122058466076851)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.0698944479227066)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.14631778001785278)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.2594446837902069)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.1964341104030609)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.21334423124790192)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.13131268322467804)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.26021328568458557)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.1441626250743866)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.12460647523403168)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.23581501841545105)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.20753739774227142)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.21815378963947296)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.17650839686393738)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.20608964562416077)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.10834101587533951)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.19784437119960785)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.16106487810611725)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.15117500722408295)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.12896454334259033)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.3395216464996338)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.17772121727466583)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.2523779571056366)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.21405598521232605)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.19904261827468872)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.18432344496250153)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.191059410572052)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.3724207282066345)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.1027054712176323)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.36679986119270325)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.2681431472301483)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.3599039316177368)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.2979850769042969)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.20312322676181793)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.14973033964633942)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.168622225522995)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.1715823858976364)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.42331984639167786)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.22941118478775024)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.3255874216556549)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.45013710856437683)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.13491101562976837)\n",
            "Training set: Average loss: 0.210973\n",
            "Validation set: Average loss: 0.25531643171124396, Accuracy: 9093/10000 (90.93%)\n",
            "Epoch: 9\n",
            "Training set:0/60000 (0.0 Loss:0.2651362717151642)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.13213545083999634)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.13982656598091125)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.30646294355392456)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.38233160972595215)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.11938871443271637)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.16452880203723907)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.38375529646873474)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.10713668167591095)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.17949636280536652)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.2785099744796753)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.41145145893096924)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.06322304904460907)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.1442197859287262)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.30862775444984436)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.19352230429649353)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.1742505580186844)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.16838622093200684)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.1901015192270279)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.07465580105781555)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.18342845141887665)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.2610563635826111)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.21325738728046417)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.14539046585559845)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.28425419330596924)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.24963679909706116)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.2743571698665619)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.3543740510940552)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.18824756145477295)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.17863234877586365)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.08086823672056198)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.23252853751182556)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.24132603406906128)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.16038361191749573)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.1827843189239502)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.16991053521633148)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.14622458815574646)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.2057385891675949)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.2510092556476593)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.2506190836429596)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.29146718978881836)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.09594494104385376)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.14249923825263977)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.15136821568012238)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.14511874318122864)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.21642686426639557)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.1267079859972)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.13347604870796204)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.22512955963611603)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.1370752900838852)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.19047048687934875)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.2688179910182953)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.17818447947502136)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.26671457290649414)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.12401029467582703)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.13248994946479797)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.0944940373301506)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.23633180558681488)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.1868162453174591)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.1609167456626892)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.1248394325375557)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.11410724371671677)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.19714906811714172)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.28799188137054443)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.10159078985452652)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.14997120201587677)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.21690498292446136)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.16363288462162018)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.16622719168663025)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.2552819848060608)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.4596322774887085)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.06836368143558502)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.19469942152500153)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.11933121830224991)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.1326148957014084)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.3041996955871582)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.1458677053451538)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.16607603430747986)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.21671555936336517)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.19581888616085052)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.0462048165500164)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.2111411988735199)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.4198800325393677)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.20302920043468475)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.16984787583351135)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.1060493215918541)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.31997057795524597)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.17466451227664948)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.18048767745494843)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.2077573537826538)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.14932261407375336)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.300467848777771)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.07749571651220322)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.085051029920578)\n",
            "Training set: Average loss: 0.203229\n",
            "Validation set: Average loss: 0.2588500740706541, Accuracy: 9119/10000 (91.19%)\n",
            "Epoch: 10\n",
            "Training set:0/60000 (0.0 Loss:0.23909781873226166)\n",
            "Training set:640/60000 (1.0660980810234542 Loss:0.3325420618057251)\n",
            "Training set:1280/60000 (2.1321961620469083 Loss:0.16935749351978302)\n",
            "Training set:1920/60000 (3.1982942430703623 Loss:0.2047620415687561)\n",
            "Training set:2560/60000 (4.264392324093817 Loss:0.22404734790325165)\n",
            "Training set:3200/60000 (5.330490405117271 Loss:0.15849806368350983)\n",
            "Training set:3840/60000 (6.3965884861407245 Loss:0.10169118642807007)\n",
            "Training set:4480/60000 (7.462686567164179 Loss:0.18098989129066467)\n",
            "Training set:5120/60000 (8.528784648187633 Loss:0.11117585003376007)\n",
            "Training set:5760/60000 (9.594882729211088 Loss:0.1577921211719513)\n",
            "Training set:6400/60000 (10.660980810234541 Loss:0.16260281205177307)\n",
            "Training set:7040/60000 (11.727078891257996 Loss:0.1291395127773285)\n",
            "Training set:7680/60000 (12.793176972281449 Loss:0.1645076721906662)\n",
            "Training set:8320/60000 (13.859275053304904 Loss:0.28773069381713867)\n",
            "Training set:8960/60000 (14.925373134328359 Loss:0.13083596527576447)\n",
            "Training set:9600/60000 (15.991471215351812 Loss:0.11275500804185867)\n",
            "Training set:10240/60000 (17.057569296375267 Loss:0.169916033744812)\n",
            "Training set:10880/60000 (18.12366737739872 Loss:0.16928689181804657)\n",
            "Training set:11520/60000 (19.189765458422176 Loss:0.2706121802330017)\n",
            "Training set:12160/60000 (20.255863539445627 Loss:0.24285271763801575)\n",
            "Training set:12800/60000 (21.321961620469082 Loss:0.26363012194633484)\n",
            "Training set:13440/60000 (22.388059701492537 Loss:0.14959892630577087)\n",
            "Training set:14080/60000 (23.454157782515992 Loss:0.22986017167568207)\n",
            "Training set:14720/60000 (24.520255863539447 Loss:0.14923322200775146)\n",
            "Training set:15360/60000 (25.586353944562898 Loss:0.10974190384149551)\n",
            "Training set:16000/60000 (26.652452025586353 Loss:0.2610406279563904)\n",
            "Training set:16640/60000 (27.718550106609808 Loss:0.1154022291302681)\n",
            "Training set:17280/60000 (28.784648187633262 Loss:0.13579338788986206)\n",
            "Training set:17920/60000 (29.850746268656717 Loss:0.1738492101430893)\n",
            "Training set:18560/60000 (30.916844349680172 Loss:0.21429963409900665)\n",
            "Training set:19200/60000 (31.982942430703623 Loss:0.2778266370296478)\n",
            "Training set:19840/60000 (33.04904051172708 Loss:0.08746692538261414)\n",
            "Training set:20480/60000 (34.11513859275053 Loss:0.13980084657669067)\n",
            "Training set:21120/60000 (35.18123667377399 Loss:0.15363407135009766)\n",
            "Training set:21760/60000 (36.24733475479744 Loss:0.13534195721149445)\n",
            "Training set:22400/60000 (37.3134328358209 Loss:0.1937183439731598)\n",
            "Training set:23040/60000 (38.37953091684435 Loss:0.15534497797489166)\n",
            "Training set:23680/60000 (39.44562899786781 Loss:0.1145169660449028)\n",
            "Training set:24320/60000 (40.511727078891255 Loss:0.12236588448286057)\n",
            "Training set:24960/60000 (41.57782515991471 Loss:0.19593334197998047)\n",
            "Training set:25600/60000 (42.643923240938165 Loss:0.1741977334022522)\n",
            "Training set:26240/60000 (43.71002132196162 Loss:0.08392958343029022)\n",
            "Training set:26880/60000 (44.776119402985074 Loss:0.14432622492313385)\n",
            "Training set:27520/60000 (45.84221748400853 Loss:0.15028217434883118)\n",
            "Training set:28160/60000 (46.908315565031984 Loss:0.09385359287261963)\n",
            "Training set:28800/60000 (47.97441364605544 Loss:0.17105035483837128)\n",
            "Training set:29440/60000 (49.04051172707889 Loss:0.14389503002166748)\n",
            "Training set:30080/60000 (50.10660980810235 Loss:0.32449454069137573)\n",
            "Training set:30720/60000 (51.172707889125796 Loss:0.253950834274292)\n",
            "Training set:31360/60000 (52.23880597014925 Loss:0.2580866813659668)\n",
            "Training set:32000/60000 (53.304904051172706 Loss:0.21819402277469635)\n",
            "Training set:32640/60000 (54.37100213219616 Loss:0.15750820934772491)\n",
            "Training set:33280/60000 (55.437100213219615 Loss:0.2330009937286377)\n",
            "Training set:33920/60000 (56.50319829424307 Loss:0.20504635572433472)\n",
            "Training set:34560/60000 (57.569296375266525 Loss:0.21863125264644623)\n",
            "Training set:35200/60000 (58.63539445628998 Loss:0.39690589904785156)\n",
            "Training set:35840/60000 (59.701492537313435 Loss:0.2571583092212677)\n",
            "Training set:36480/60000 (60.76759061833689 Loss:0.2288271188735962)\n",
            "Training set:37120/60000 (61.833688699360344 Loss:0.1338755488395691)\n",
            "Training set:37760/60000 (62.89978678038379 Loss:0.13979977369308472)\n",
            "Training set:38400/60000 (63.96588486140725 Loss:0.16908212006092072)\n",
            "Training set:39040/60000 (65.0319829424307 Loss:0.12221412360668182)\n",
            "Training set:39680/60000 (66.09808102345416 Loss:0.27895382046699524)\n",
            "Training set:40320/60000 (67.16417910447761 Loss:0.20473869144916534)\n",
            "Training set:40960/60000 (68.23027718550107 Loss:0.10212646424770355)\n",
            "Training set:41600/60000 (69.29637526652452 Loss:0.1263008713722229)\n",
            "Training set:42240/60000 (70.36247334754798 Loss:0.15312781929969788)\n",
            "Training set:42880/60000 (71.42857142857143 Loss:0.25891387462615967)\n",
            "Training set:43520/60000 (72.49466950959489 Loss:0.3448444604873657)\n",
            "Training set:44160/60000 (73.56076759061834 Loss:0.24083904922008514)\n",
            "Training set:44800/60000 (74.6268656716418 Loss:0.29960188269615173)\n",
            "Training set:45440/60000 (75.69296375266525 Loss:0.2468135505914688)\n",
            "Training set:46080/60000 (76.7590618336887 Loss:0.32158589363098145)\n",
            "Training set:46720/60000 (77.82515991471216 Loss:0.1784573346376419)\n",
            "Training set:47360/60000 (78.89125799573561 Loss:0.10327943414449692)\n",
            "Training set:48000/60000 (79.95735607675905 Loss:0.26928824186325073)\n",
            "Training set:48640/60000 (81.02345415778251 Loss:0.145426943898201)\n",
            "Training set:49280/60000 (82.08955223880596 Loss:0.32067427039146423)\n",
            "Training set:49920/60000 (83.15565031982942 Loss:0.28600409626960754)\n",
            "Training set:50560/60000 (84.22174840085287 Loss:0.2488763928413391)\n",
            "Training set:51200/60000 (85.28784648187633 Loss:0.23692582547664642)\n",
            "Training set:51840/60000 (86.35394456289978 Loss:0.15895695984363556)\n",
            "Training set:52480/60000 (87.42004264392324 Loss:0.21317344903945923)\n",
            "Training set:53120/60000 (88.4861407249467 Loss:0.24208222329616547)\n",
            "Training set:53760/60000 (89.55223880597015 Loss:0.13510878384113312)\n",
            "Training set:54400/60000 (90.6183368869936 Loss:0.13874751329421997)\n",
            "Training set:55040/60000 (91.68443496801706 Loss:0.2948022782802582)\n",
            "Training set:55680/60000 (92.75053304904051 Loss:0.3195323050022125)\n",
            "Training set:56320/60000 (93.81663113006397 Loss:0.20047730207443237)\n",
            "Training set:56960/60000 (94.88272921108742 Loss:0.19871482253074646)\n",
            "Training set:57600/60000 (95.94882729211088 Loss:0.14633169770240784)\n",
            "Training set:58240/60000 (97.01492537313433 Loss:0.17952647805213928)\n",
            "Training set:58880/60000 (98.08102345415779 Loss:0.19603542983531952)\n",
            "Training set:59520/60000 (99.14712153518124 Loss:0.13776011765003204)\n",
            "Training set: Average loss: 0.195582\n",
            "Validation set: Average loss: 0.26345833585520456, Accuracy: 9104/10000 (91.04%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "-1OVpnVd9Q-a",
        "outputId": "ead78139-e9fe-4369-bce2-b917f11f9b46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7CwgVe7Fyus",
        "outputId": "8b2de3e0-6d89-41fa-d7c9-e7739c191d1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 792038\n"
          ]
        }
      ]
    }
  ]
}