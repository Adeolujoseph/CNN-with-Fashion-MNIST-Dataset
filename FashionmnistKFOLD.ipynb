{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJQJeHR8asmYorUi/JohSt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import ConcatDataset\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "kh0qGRuWQKwv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Load the  dataset without normalization\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
        "\n",
        "# Calculate mean and std across the dataset\n",
        "data = train_dataset.data/ 255.  # Scale pixel values to [0, 1]\n",
        "mean = torch.mean(data)\n",
        "std= torch.std(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std Deviation:\", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd7EzCTB60PL",
        "outputId": "90f99f5c-bcc2-4779-e19b-db40ac8c4e21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8518787.22it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 66234.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:08<00:00, 499460.34it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18940593.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Mean: tensor(0.2860)\n",
            "Std Deviation: tensor(0.3530)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the MNIST dataset without normalization\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Calculate mean and std across the dataset\n",
        "data = test_dataset.data/ 255.  # Scale pixel values to [0, 1]\n",
        "mean = torch.mean(data)\n",
        "std= torch.std(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std Deviation:\", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVM18T9P62b7",
        "outputId": "10dbc359-1962-4d08-ced1-542ef84dd8b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor(0.2868)\n",
            "Std Deviation: tensor(0.3524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    # Add other transformations as needed (resize, normalization, etc.)\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Combine both datasets\n",
        "combined_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
        "\n",
        "# Collect all tensors into a list\n",
        "tensor_list = []\n",
        "for i in range(len(combined_dataset)):\n",
        "    tensor_list.append(combined_dataset[i][0])  # Assuming the first element of each sample is the tensor/image\n",
        "\n",
        "# Stack the tensors into a single tensor\n",
        "combined_tensor = torch.stack(tensor_list)\n",
        "\n",
        "# Check the type and shape of the combined tensor\n",
        "print(type(combined_tensor))\n",
        "print(combined_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BbUlnzX8Tmn",
        "outputId": "1bcbd8ea-100c-44f2-839f-eed366f3d52f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([70000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural net class\n",
        "class Net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Our images are grayscale, so input channels = 1. We'll apply 48 filters in the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=48, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        # We'll apply max pooling with a kernel size of 2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A second convolutional layer takes 48 input channels, and generates 96 outputs\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        # We'll apply another max pooling with a kernel size of 2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # A drop layer deletes 30% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.3)\n",
        "\n",
        "        # Our 28x28 image tensors will be pooled twice with a kernel size of 2. 28/2/2 is 7.\n",
        "        # So our feature tensors are now 7 x 7, and we've generated 96 of them\n",
        "        # We need to flatten these and feed them to a fully-connected layer\n",
        "        # to map them to  the probability for each class\n",
        "        self.fc = nn.Linear(in_features=7 * 7 * 96, out_features=164)\n",
        "        self.fc2 = nn.Linear(in_features=164, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use a relu activation function after layer 1 (convolution 1 and pool)\n",
        "        x = F.relu(self.pool1(self.conv1(x)))\n",
        "\n",
        "        # Use a relu activation function after layer 2 (convolution 2 and pool)\n",
        "        x = F.relu(self.pool2(self.conv2(x)))\n",
        "\n",
        "        # Select some features to drop after the 2nd convolution to prevent overfitting\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 7 * 7 * 96)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x",
        "# Instantiate the model\n",
        "model = Net()\n",
        "print(\"CNN model class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AbQvab-QKKe",
        "outputId": "802c9f29-d2c8-4537-9709-d0d2370c2274"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Training set:{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)} Loss:{loss.item()})')\n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "JaC919KsROws"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    accuracy = correct / len(val_indices)  # Using the length of the validation subset loader\n",
        "    print(f'Validation set: Average loss: {avg_loss}, Accuracy: {correct}/{len(val_indices)} ({100. * accuracy}%)')\n",
        "   # return average loss for the epoch\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "j5w--8AZRSS2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Define k value for k-fold cross-validation\n",
        "k = 7  # You can set the desired number of folds\n",
        "\n",
        "# Initialize k-fold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "batch_size = 64\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "fold_accuracies = []  # Track accuracy for each fold\n",
        "test_losses = []\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(combined_tensor)):\n",
        "    print(f\"Fold {fold + 1}/{k}\")\n",
        "\n",
        "    # Create data loaders for this fold\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "    fold_train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    fold_val_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Reinitialize model for each fold\n",
        "    model = Net(num_classes=10).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    epochs = 10\n",
        "\n",
        "    # Track accuracy for this fold\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Train the model for this fold\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, fold_train_loader, optimizer, epoch)\n",
        "        val_loss = test(model, device, fold_val_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(val_loss)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    with torch.no_grad():\n",
        "        for data, target in fold_val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    fold_accuracy = 100 * correct / total\n",
        "    fold_accuracies.append(fold_accuracy)\n",
        "    print(f\"Accuracy for fold {fold + 1}: {fold_accuracy:.2f}%\")\n",
        "\n",
        "# Calculate average accuracy across all folds\n",
        "avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "print(f\"Average accuracy across all folds: {avg_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uABG6eZoEFc",
        "outputId": "18f55725-b391-41a4-c534-909272cdef76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.19694341719150543)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.16531775891780853)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.20398035645484924)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.22785986959934235)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2663370370864868)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.16028150916099548)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.17300555109977722)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.22443245351314545)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.25465911626815796)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.18372970819473267)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.18687567114830017)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.21477815508842468)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.2985050678253174)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3791079521179199)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.23586323857307434)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.14887003600597382)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.15425947308540344)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.302058607339859)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.1786252111196518)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.411140501499176)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.28166142106056213)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.1603556126356125)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.3631446361541748)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.21112585067749023)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.30821678042411804)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.23934009671211243)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.18947328627109528)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.15844546258449554)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1512216329574585)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2268366515636444)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.0966271162033081)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2002941071987152)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.36629927158355713)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.22792914509773254)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.23969028890132904)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.17499126493930817)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.27449068427085876)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.22622942924499512)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1722436398267746)\n",
            "Training set: Average loss: 0.214786\n",
            "Validation set: Average loss: 0.23604704434894452, Accuracy: 9140/10000 (91.4%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.2288682609796524)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.21057355403900146)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.1455986201763153)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.2104116827249527)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.16734063625335693)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.17528964579105377)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.24970196187496185)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2770005464553833)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.09447521716356277)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.18452104926109314)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.22697877883911133)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.3259749412536621)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.15938393771648407)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.5104638934135437)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3688494861125946)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.20002806186676025)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.269387811422348)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.18277521431446075)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.1297473907470703)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.15240605175495148)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.22518759965896606)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.250901997089386)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.14095750451087952)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.2001873403787613)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.2583436071872711)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.17525775730609894)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.23320074379444122)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.31585201621055603)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.1269310563802719)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.2395678609609604)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.21916133165359497)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.11308865994215012)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2782425284385681)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.11775372177362442)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.18949587643146515)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.15443487465381622)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1622719019651413)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.21722954511642456)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.18905408680438995)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.1219295859336853)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.09493660181760788)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.19482555985450745)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.23271042108535767)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.3504538834095001)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.1367870569229126)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.09537775814533234)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.1922009289264679)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.19156616926193237)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.23717263340950012)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2688465714454651)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.21975477039813995)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2697398364543915)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.33161360025405884)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.21285951137542725)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.20811958611011505)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.23791445791721344)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2242082953453064)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.06970468908548355)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2896256148815155)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.13754425942897797)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.13473603129386902)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.16840040683746338)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2244586944580078)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.17812533676624298)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.22677107155323029)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.23794536292552948)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3736518919467926)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.1875099092721939)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.30659353733062744)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.13659071922302246)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.19088150560855865)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.1504759043455124)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.1287623643875122)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.24081146717071533)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.14441978931427002)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3094082772731781)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.23516137897968292)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.22418293356895447)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.11630955338478088)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17791946232318878)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.1596965193748474)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.16606734693050385)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2050982117652893)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.13663813471794128)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.22433516383171082)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.16717348992824554)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2083667665719986)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.15293066203594208)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2714264690876007)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.17559707164764404)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.28574907779693604)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.20751842856407166)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2052323967218399)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1327318400144577)\n",
            "Training set: Average loss: 0.206074\n",
            "Validation set: Average loss: 0.22959966857911676, Accuracy: 9163/10000 (91.63%)\n",
            "Accuracy for fold 2: 91.63%\n",
            "Fold 3/7\n",
            "Epoch: 1\n",
            "Training set:0/70000 (0.0 Loss:2.293335199356079)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:1.7409508228302002)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.995968759059906)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.8785722851753235)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.7372239828109741)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.8174435496330261)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.8556612133979797)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.5917291641235352)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.615679144859314)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.5973075032234192)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.5269874930381775)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.7042983174324036)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.4603642225265503)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3739676773548126)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.5839219093322754)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.3979353606700897)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.46872928738594055)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.42172154784202576)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.5063721537590027)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.4205484688282013)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.36421558260917664)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.5069772005081177)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.6805999279022217)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.521912157535553)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.45595747232437134)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.5236797332763672)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.531093180179596)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.5361589789390564)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.5207633972167969)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.31126752495765686)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.4892942011356354)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.6199901700019836)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.4172419309616089)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.26943323016166687)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.46828919649124146)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.4778439700603485)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.4115194082260132)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2919795513153076)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.4586225748062134)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.4497198164463043)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.4475853145122528)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.4990770220756531)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.39296403527259827)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.44122767448425293)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.4456976056098938)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.6300168037414551)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.4656081199645996)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.18412816524505615)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.4467894434928894)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.6951931118965149)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.44370153546333313)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.3489740788936615)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.49060267210006714)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.5955561995506287)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.4894213080406189)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2782081663608551)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.33274656534194946)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2972331941127777)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.29285842180252075)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.5556398630142212)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3757826089859009)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3655616044998169)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.49220138788223267)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.20579980313777924)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.32213345170021057)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.317432165145874)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.48403990268707275)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.5977425575256348)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.18573395907878876)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.34575212001800537)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.5259614586830139)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.37658631801605225)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.30587878823280334)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2526315450668335)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.22482196986675262)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.4504532814025879)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.44862255454063416)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.34349870681762695)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3229884207248688)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.26225990056991577)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2683831453323364)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.33631792664527893)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.34402570128440857)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.5170546770095825)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.302094042301178)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.17388340830802917)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.21280138194561005)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.22277463972568512)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.26736173033714294)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.37506115436553955)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.4161147475242615)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.30362775921821594)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.30654382705688477)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.3969748318195343)\n",
            "Training set: Average loss: 0.475822\n",
            "Validation set: Average loss: 0.34322170258327656, Accuracy: 8783/10000 (87.83%)\n",
            "Epoch: 2\n",
            "Training set:0/70000 (0.0 Loss:0.37132030725479126)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.36676156520843506)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.2167772352695465)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.29381805658340454)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.5780029296875)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.35666757822036743)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.4132120609283447)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.3432575464248657)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.44966354966163635)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.44776883721351624)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.5082748532295227)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2765830457210541)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.30965742468833923)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3032025992870331)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.44348829984664917)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2253362536430359)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.23495495319366455)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.43073683977127075)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2997797727584839)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.21529076993465424)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.197487011551857)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.16617156565189362)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.37599310278892517)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.45879876613616943)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.5004569888114929)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.29412606358528137)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.3008795976638794)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.26736244559288025)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.4298129379749298)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.3605010509490967)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.45755213499069214)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.33643290400505066)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.25811880826950073)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.33122313022613525)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3494988679885864)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.34164419770240784)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.5171621441841125)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3650011718273163)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.30331555008888245)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.398141473531723)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.18584278225898743)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3715825080871582)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.3490244448184967)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.44010138511657715)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.30176639556884766)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.1956319808959961)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.34839701652526855)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2749145030975342)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.25872382521629333)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2583465278148651)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3290105164051056)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.23559479415416718)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.18511565029621124)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.24188101291656494)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3686408996582031)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.34681564569473267)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.25613275170326233)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.47583475708961487)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.3527732789516449)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.4625263512134552)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.2664036750793457)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3924929201602936)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.37137097120285034)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.4322208762168884)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.33753544092178345)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.25498491525650024)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.5801552534103394)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.3977508246898651)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.4252760410308838)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.19422753155231476)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4702754616737366)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.335419625043869)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.38606980443000793)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20090344548225403)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.24543115496635437)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.1681790053844452)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.22828440368175507)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.25663992762565613)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.48821142315864563)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.33840006589889526)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.5462743639945984)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.28580358624458313)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.41775205731391907)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.29431673884391785)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2664664685726166)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.22160197794437408)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.27982988953590393)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.21467486023902893)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.46516743302345276)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.1719404011964798)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.1871623396873474)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.23792189359664917)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.34841325879096985)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.16129618883132935)\n",
            "Training set: Average loss: 0.328538\n",
            "Validation set: Average loss: 0.3079857704745736, Accuracy: 8898/10000 (88.98%)\n",
            "Epoch: 3\n",
            "Training set:0/70000 (0.0 Loss:0.30181244015693665)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2967151403427124)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.4096711277961731)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.24858972430229187)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.46328437328338623)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.30700311064720154)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.20055927336215973)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.24279451370239258)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.29000067710876465)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2258986383676529)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.22535240650177002)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2144387662410736)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.3292669653892517)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.19788694381713867)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.22069160640239716)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.22839371860027313)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.5479221940040588)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2422986477613449)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.45464399456977844)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.21484526991844177)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.2565271258354187)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.29775291681289673)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.18718034029006958)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.2477063089609146)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.31733283400535583)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.26709631085395813)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.27655842900276184)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1136135384440422)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.22950750589370728)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.4381943643093109)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.24314278364181519)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.35356462001800537)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.45249536633491516)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2760838270187378)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2427615523338318)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.20916810631752014)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2898554503917694)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.322517454624176)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2736760973930359)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.15540415048599243)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2121317833662033)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.2772662043571472)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.33078733086586)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2685011923313141)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.2779882550239563)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.35405123233795166)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.32277441024780273)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.3878163695335388)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3288816511631012)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.1544192135334015)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.33786284923553467)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.22299277782440186)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.22506389021873474)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.3046872913837433)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2036104053258896)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.29832723736763)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2974579930305481)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.27015969157218933)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.27551954984664917)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2805574834346771)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.23292356729507446)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3136352002620697)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2143091857433319)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.30282941460609436)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.33988749980926514)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.24056217074394226)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.28905612230300903)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.2716090977191925)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2485935240983963)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2361752837896347)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.5363322496414185)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.2240220308303833)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.24999837577342987)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2490779459476471)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.17427502572536469)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.27673524618148804)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.35028746724128723)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.3268924653530121)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3716917634010315)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2487713098526001)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.27463626861572266)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3630891740322113)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.16658058762550354)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.2579777240753174)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.4882182776927948)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.36320173740386963)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2503092885017395)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.29159456491470337)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.19967277348041534)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2571113407611847)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.24293813109397888)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.18754182755947113)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.30683770775794983)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2564781904220581)\n",
            "Training set: Average loss: 0.292560\n",
            "Validation set: Average loss: 0.284188940693998, Accuracy: 9005/10000 (90.05%)\n",
            "Epoch: 4\n",
            "Training set:0/70000 (0.0 Loss:0.3342635929584503)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.25680404901504517)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.377931147813797)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.16114437580108643)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.19948433339595795)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.22426505386829376)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.3262448012828827)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2198382169008255)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.32556799054145813)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1819014847278595)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.27425816655158997)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.22881188988685608)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.15243017673492432)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.2279510498046875)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.236632838845253)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.15041469037532806)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.23345443606376648)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.20534968376159668)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.21518540382385254)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2507931888103485)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.3123185336589813)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.1939510703086853)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.21921509504318237)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.24674232304096222)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.42578884959220886)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2124629020690918)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.14730754494667053)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.25915640592575073)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.22863589227199554)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.29215866327285767)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.24047790467739105)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.25201135873794556)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.18096527457237244)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.3140217065811157)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3052939474582672)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.37324076890945435)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.33393773436546326)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.21546627581119537)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.26679959893226624)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.2415679395198822)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2231007218360901)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.22007016837596893)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.3381326496601105)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.19465860724449158)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.24859756231307983)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.17491106688976288)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.2947333753108978)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.34496068954467773)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.31604084372520447)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.22977091372013092)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.5125321745872498)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.29143041372299194)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.1834198534488678)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.24187566339969635)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.12349018454551697)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.383131206035614)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2609350085258484)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.12137696146965027)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.30481377243995667)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.34269797801971436)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.25181618332862854)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.16429105401039124)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2730354070663452)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.1914384663105011)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.4036795198917389)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.14565086364746094)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.42431628704071045)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.4729301631450653)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2148291915655136)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.22197452187538147)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.23331113159656525)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3298828601837158)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.24134252965450287)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.21115359663963318)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.26465725898742676)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.24454373121261597)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.30998215079307556)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1863837093114853)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.32959845662117004)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.27114275097846985)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.3033023476600647)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.21487130224704742)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.13308222591876984)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.18471676111221313)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2613968849182129)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.37865206599235535)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.36664456129074097)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.15286076068878174)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.18614767491817474)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2906046211719513)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.2718009054660797)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.49729034304618835)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.18031568825244904)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.3168652653694153)\n",
            "Training set: Average loss: 0.264955\n",
            "Validation set: Average loss: 0.2859358279283639, Accuracy: 8948/10000 (89.48%)\n",
            "Epoch: 5\n",
            "Training set:0/70000 (0.0 Loss:0.31882765889167786)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.21570003032684326)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.1732892543077469)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.18713749945163727)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.20160940289497375)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.29572221636772156)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.43165624141693115)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.21726301312446594)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.274921178817749)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.21136899292469025)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.1423756629228592)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.21107959747314453)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2889140248298645)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.27657851576805115)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3037579357624054)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2982885539531708)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2173679769039154)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.23910988867282867)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.26946350932121277)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2226470708847046)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.22275671362876892)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.38920968770980835)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.29686179757118225)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.23701773583889008)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.2313310205936432)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2823813259601593)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.2834957540035248)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.36909809708595276)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.15895213186740875)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.22264017164707184)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.17527224123477936)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2685929536819458)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.30409887433052063)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.21558423340320587)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.21345697343349457)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.11967217922210693)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.23994234204292297)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2504454255104065)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.27290910482406616)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.10416489839553833)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.12473645061254501)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.27205193042755127)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.1743791252374649)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.26026827096939087)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.43559151887893677)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.2912098169326782)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.36458420753479004)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.18902790546417236)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.24616709351539612)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.25604331493377686)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.260446161031723)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.07236386090517044)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.3109457194805145)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.14557147026062012)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2991851270198822)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.23024125397205353)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.09110124409198761)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.16125261783599854)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2700049579143524)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.23783190548419952)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.22007569670677185)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.5428261756896973)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.20888228714466095)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2221885472536087)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2508409023284912)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.10859039425849915)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.332485169172287)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.16175132989883423)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.15131421387195587)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.21106046438217163)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.175140380859375)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.14341029524803162)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.24720720946788788)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20029529929161072)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.14544528722763062)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2908027172088623)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.29192379117012024)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.15539875626564026)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.24780072271823883)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2970856726169586)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.12107536941766739)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.20817214250564575)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.20595282316207886)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.48710083961486816)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.23353311419487)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3291483223438263)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2778626084327698)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.23336757719516754)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.39370256662368774)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2236296534538269)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.30073609948158264)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.174096018075943)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.1371987760066986)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2020817995071411)\n",
            "Training set: Average loss: 0.248720\n",
            "Validation set: Average loss: 0.28203115574303705, Accuracy: 8996/10000 (89.96%)\n",
            "Epoch: 6\n",
            "Training set:0/70000 (0.0 Loss:0.3693821132183075)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.17778891324996948)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.19972971081733704)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.17482468485832214)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.15650951862335205)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.16449707746505737)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.34562259912490845)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.21988911926746368)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2720789611339569)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2692944407463074)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.15612567961215973)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.1466701477766037)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.08518894761800766)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.12899644672870636)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.36991631984710693)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2211722731590271)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2486356645822525)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2574324607849121)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.3118250072002411)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.16384299099445343)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.4369976222515106)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.20715925097465515)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.24070695042610168)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.24112743139266968)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.11757724732160568)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.31373468041419983)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.19020824134349823)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.25745290517807007)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.2576883137226105)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.1696738302707672)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.21735645830631256)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2578590512275696)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.24820922315120697)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.23042094707489014)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.26409491896629333)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.14127704501152039)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1561175435781479)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.20563563704490662)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.13521382212638855)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.15095818042755127)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.21357287466526031)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.28513139486312866)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.27245962619781494)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2762989401817322)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.21731622517108917)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.20353838801383972)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.15590855479240417)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.17953038215637207)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.25156664848327637)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.4214800298213959)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.40400320291519165)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.22471380233764648)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.26562681794166565)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2762836515903473)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.20823490619659424)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2087806612253189)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2283400148153305)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.11415773630142212)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2710707187652588)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.14667417109012604)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.5053468346595764)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.07305566966533661)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.19194309413433075)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.41407057642936707)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.15430736541748047)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.15681776404380798)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.21326503157615662)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.3409760296344757)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.290317565202713)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2142312228679657)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.25053495168685913)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.20052705705165863)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.22727927565574646)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2446213662624359)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.26557016372680664)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.26013314723968506)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.27866724133491516)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2417684942483902)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19002537429332733)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17810772359371185)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.28256863355636597)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.4121094346046448)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3539825975894928)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.19920119643211365)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.19273054599761963)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.37260910868644714)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2966873347759247)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.2770291566848755)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2504885196685791)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.22022388875484467)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.25500744581222534)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.20074650645256042)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.23867900669574738)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.14242900907993317)\n",
            "Training set: Average loss: 0.235068\n",
            "Validation set: Average loss: 0.2757309092457887, Accuracy: 9036/10000 (90.36%)\n",
            "Epoch: 7\n",
            "Training set:0/70000 (0.0 Loss:0.2827444076538086)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.10029298067092896)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.16938363015651703)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.19803977012634277)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.07379476726055145)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.11875022202730179)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.10382518917322159)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.13366976380348206)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2950761020183563)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2818071246147156)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.26763054728507996)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.382117360830307)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2745705544948578)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.2309301197528839)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3091408312320709)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.1514388769865036)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.12332738935947418)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.26281896233558655)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.16912445425987244)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.13398201763629913)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.1347857415676117)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.1297372728586197)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.19880615174770355)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.14725682139396667)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.11181820929050446)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.17893539369106293)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.1636553853750229)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1801314651966095)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3481493890285492)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.0915876030921936)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.15111371874809265)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.1924533098936081)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.307749480009079)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.3132498860359192)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.12403954565525055)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.17337846755981445)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2114849090576172)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.11773993819952011)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.20237544178962708)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.26583680510520935)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.21247048676013947)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.22051730751991272)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.07656477391719818)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.1804310828447342)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.12681740522384644)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.36515012383461)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.29036271572113037)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2347114384174347)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2863194942474365)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.17992183566093445)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.18252217769622803)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.30946263670921326)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2000729739665985)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2504868507385254)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.20904001593589783)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.11230470985174179)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.10264428704977036)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.11765143275260925)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.36154040694236755)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.26149648427963257)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.1385127156972885)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3600251376628876)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.27156856656074524)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.44667017459869385)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.423180490732193)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.08248908817768097)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.22521109879016876)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.2679651379585266)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.18390896916389465)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.38360491394996643)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.18902689218521118)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.30014052987098694)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.33245447278022766)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.1945675164461136)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2506133019924164)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.18642790615558624)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2668830454349518)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.09539519250392914)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19833500683307648)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.10222215205430984)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2824583351612091)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.26418784260749817)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.24961015582084656)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.13944530487060547)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.38740667700767517)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.21777477860450745)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.1125340685248375)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.1733749806880951)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.13331136107444763)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.22299712896347046)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.17092379927635193)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.13844722509384155)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.3137054443359375)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.09602604061365128)\n",
            "Training set: Average loss: 0.221922\n",
            "Validation set: Average loss: 0.2611725137681718, Accuracy: 9074/10000 (90.74%)\n",
            "Epoch: 8\n",
            "Training set:0/70000 (0.0 Loss:0.26238131523132324)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2108873426914215)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.10211250185966492)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.17878282070159912)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14217735826969147)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.14559368789196014)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.11698578298091888)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.18153494596481323)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.21928530931472778)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.4641730487346649)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3700335621833801)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.20689314603805542)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.16608545184135437)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.26658836007118225)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.15365546941757202)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.23599138855934143)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.17423653602600098)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.19331900775432587)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.44018566608428955)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.29854971170425415)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.08507068455219269)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.1740751564502716)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.21181392669677734)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.07526560127735138)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.15985895693302155)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.1170181930065155)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.13041044771671295)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.26536357402801514)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3241114318370819)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.15404734015464783)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.1422775387763977)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.08081846684217453)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.3619195222854614)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.26904940605163574)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.16271236538887024)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.32241493463516235)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.14770542085170746)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.35463276505470276)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.17719164490699768)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.39919012784957886)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.11804137378931046)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.10128083825111389)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.11819464713335037)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.16657419502735138)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.1439071148633957)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.3163018524646759)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.23508137464523315)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.14806456863880157)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2147304117679596)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.12895086407661438)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.15637008845806122)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.24532237648963928)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.29005950689315796)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.24236798286437988)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.14574824273586273)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.19589278101921082)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.15383894741535187)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.17894206941127777)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.23006172478199005)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.21614915132522583)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3607725203037262)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2297738939523697)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.28605449199676514)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.1978425234556198)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.265164315700531)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.24083702266216278)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.14046207070350647)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.25230368971824646)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2400352507829666)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2127639651298523)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.13545997440814972)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.1670946180820465)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3438805043697357)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.15985187888145447)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.18311135470867157)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.24193432927131653)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2629995048046112)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2906862199306488)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.16777876019477844)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3974638879299164)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.24726685881614685)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.21074694395065308)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.15912678837776184)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1547536700963974)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2934614419937134)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.14358340203762054)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.11905518919229507)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.12926970422267914)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.13709789514541626)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.24526150524616241)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.10424506664276123)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.11538873612880707)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.16064974665641785)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1802627444267273)\n",
            "Training set: Average loss: 0.215372\n",
            "Validation set: Average loss: 0.26009795325956525, Accuracy: 9092/10000 (90.92%)\n",
            "Epoch: 9\n",
            "Training set:0/70000 (0.0 Loss:0.34194469451904297)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.25549858808517456)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.28118228912353516)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.1928197741508484)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14817537367343903)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.283573180437088)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.18193960189819336)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.1274777203798294)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.12571167945861816)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1402583122253418)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3155694305896759)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.1874264031648636)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.18603098392486572)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.15096287429332733)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.10499768704175949)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.24364899098873138)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.19292470812797546)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.143692746758461)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.12396039813756943)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.1388164460659027)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.26415833830833435)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.19179093837738037)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.1963982880115509)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.19253407418727875)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.20916298031806946)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.09948623925447464)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.23489679396152496)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.14259174466133118)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.28336605429649353)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.24223433434963226)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.10012146085500717)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.432749480009079)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.17585060000419617)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.349174439907074)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.14602495729923248)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.24270421266555786)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.34980812668800354)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2678290605545044)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.3070457875728607)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.3296675682067871)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.06963960081338882)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.13272906839847565)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.27448081970214844)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.24586276710033417)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.16269336640834808)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.18625809252262115)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.255144327878952)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.3708433210849762)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.16842451691627502)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.10453411191701889)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.16466422379016876)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.208215594291687)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2731829583644867)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.19758963584899902)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.08849169313907623)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.19041241705417633)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.19373053312301636)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.19066475331783295)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.14297479391098022)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3748318552970886)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.28877535462379456)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.12218426167964935)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.16961312294006348)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.15999968349933624)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.19301655888557434)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.17715628445148468)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.15674453973770142)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.08580867946147919)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.21477407217025757)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.323562353849411)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.21895474195480347)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.33631008863449097)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.11422839015722275)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.15373769402503967)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.10955996066331863)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.20150132477283478)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.3171781897544861)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.11627021431922913)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.17858237028121948)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3275770843029022)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.16654308140277863)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2016872763633728)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.346209853887558)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.13109055161476135)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2706683576107025)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3458559513092041)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2283775508403778)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.15662561357021332)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.14859318733215332)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2082749456167221)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.17901302874088287)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.28918522596359253)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.1941579282283783)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1905452013015747)\n",
            "Training set: Average loss: 0.206029\n",
            "Validation set: Average loss: 0.25607311837137886, Accuracy: 9117/10000 (91.17%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.2451106309890747)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.149410679936409)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.1382865011692047)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.33951473236083984)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14947529137134552)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.17626868188381195)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.1456848829984665)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.10106836259365082)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.28379741311073303)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.15375228226184845)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.11714568734169006)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.17102982103824615)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.26227614283561707)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.1774100959300995)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.1994597613811493)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.14650674164295197)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.24635177850723267)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2109367698431015)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.06329870223999023)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.23577550053596497)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.0948524922132492)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.10798744112253189)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.19287094473838806)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.14806212484836578)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.09838774800300598)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.10521136224269867)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.23138020932674408)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.15357659757137299)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.2309836894273758)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.1950405091047287)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.3134782910346985)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.23082518577575684)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.21665072441101074)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2910458743572235)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.22946223616600037)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.19097110629081726)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.13864120841026306)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.14173318445682526)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2219042181968689)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.25443875789642334)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2667701840400696)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.12132223695516586)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.22147653996944427)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.20313844084739685)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.25688794255256653)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.21103328466415405)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3000100553035736)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.24031509459018707)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3339575529098511)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.22386036813259125)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.26427727937698364)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.20107312500476837)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.17450709640979767)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.25335192680358887)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2249094545841217)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.23001497983932495)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.1293247491121292)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.07207893580198288)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.21854987740516663)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2834487557411194)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.15469804406166077)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2101467251777649)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.10276716202497482)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.21307149529457092)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.1600598394870758)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.20943401753902435)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.10907378792762756)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.11535459011793137)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.26489460468292236)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.17308256030082703)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.15066510438919067)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.15417347848415375)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.1049572080373764)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.16746222972869873)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.11332184821367264)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2076093554496765)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.15644896030426025)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.39830127358436584)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.2920069694519043)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.16394513845443726)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.3482368290424347)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.27623358368873596)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.20331138372421265)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1839294582605362)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.252169132232666)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.1596798300743103)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.14690560102462769)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.3244949281215668)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2304450124502182)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.12929682433605194)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.1963777393102646)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.22572040557861328)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.21286259591579437)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.22803518176078796)\n",
            "Training set: Average loss: 0.198948\n",
            "Validation set: Average loss: 0.268494257383096, Accuracy: 9082/10000 (90.82000000000001%)\n",
            "Accuracy for fold 3: 90.82%\n",
            "Fold 4/7\n",
            "Epoch: 1\n",
            "Training set:0/70000 (0.0 Loss:2.3224918842315674)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:1.3720415830612183)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.9912822246551514)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.8904004096984863)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.6755929589271545)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.6702439785003662)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.47405365109443665)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.7930812239646912)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.5871374607086182)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.6423159837722778)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.5249466896057129)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.48663008213043213)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.39007917046546936)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.6477584838867188)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.31296902894973755)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.33414310216903687)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.5525575280189514)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.529438316822052)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.5111039280891418)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.5163456797599792)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.5478220582008362)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3963969051837921)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3636438548564911)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.5414071679115295)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.4402382969856262)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.43639951944351196)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.36818432807922363)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.28271645307540894)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3560733199119568)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.47918790578842163)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.4108578860759735)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.4410824477672577)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.4737604260444641)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.47436168789863586)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.5663884282112122)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.3952083885669708)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.36328014731407166)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3186042010784149)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.3739595115184784)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.22897133231163025)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.41259685158729553)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.46809062361717224)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.3358244299888611)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2847733199596405)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.41398948431015015)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.3136470913887024)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3454757034778595)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.315986305475235)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3643716275691986)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.44071829319000244)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.42173582315444946)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.3342492878437042)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.42353206872940063)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.29777926206588745)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.26816433668136597)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.6495175361633301)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.23259052634239197)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.3255676031112671)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.4058137536048889)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.46993082761764526)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.21212291717529297)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2630186080932617)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.3502722680568695)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.40471383929252625)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.24761459231376648)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.2740175127983093)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2889808714389801)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.40597566962242126)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2573496401309967)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.21931453049182892)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4000168740749359)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.41227179765701294)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.21574154496192932)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.29511672258377075)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.5246335864067078)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3090270757675171)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.3035339117050171)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.4759294390678406)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.49058812856674194)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.27309003472328186)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.34944969415664673)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3753271698951721)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3911967873573303)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.39283859729766846)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.33535099029541016)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.2699515223503113)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.4014734923839569)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.46194547414779663)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.4027033746242523)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.5047630667686462)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.5145937204360962)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.4486537575721741)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24903182685375214)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.3196229636669159)\n",
            "Training set: Average loss: 0.458867\n",
            "Validation set: Average loss: 0.36290055693714485, Accuracy: 8695/10000 (86.95%)\n",
            "Epoch: 2\n",
            "Training set:0/70000 (0.0 Loss:0.42379435896873474)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.3190171718597412)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.34131279587745667)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.4172617197036743)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.379868745803833)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.32320141792297363)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.37433698773384094)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2812035083770752)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.3408830463886261)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3492952585220337)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.23087769746780396)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.3849969208240509)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2878768742084503)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3694044053554535)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3592568039894104)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.20004330575466156)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2842118442058563)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3677746653556824)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.5170055031776428)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.3884746730327606)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.28058671951293945)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3165397644042969)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3032369613647461)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.30204710364341736)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3039977252483368)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.33026084303855896)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.31706807017326355)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.42827677726745605)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3784550130367279)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.3880050480365753)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2863187789916992)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.3171321153640747)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.3682759404182434)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.42766764760017395)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.421724408864975)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.22134731709957123)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2467346489429474)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3268793523311615)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.40539929270744324)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.3949948847293854)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.4248961806297302)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.29903092980384827)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.20278681814670563)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.1971016228199005)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.4219323694705963)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.2459368109703064)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3933064639568329)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.196043461561203)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3279413878917694)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.22811992466449738)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.5024745464324951)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.6118725538253784)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.33720871806144714)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.35498714447021484)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3668728768825531)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.279817670583725)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.32121899724006653)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.19183985888957977)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.28946009278297424)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.29143446683883667)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.5747929811477661)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.27904972434043884)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.25006359815597534)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.37421220541000366)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.34374532103538513)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.40805771946907043)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.5131464600563049)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.19971562922000885)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3190513551235199)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.3388250768184662)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4715275168418884)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3705359101295471)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2695309817790985)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.21533530950546265)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.32312172651290894)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.21869851648807526)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2649292051792145)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.23713241517543793)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.29557162523269653)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.316656231880188)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2625479996204376)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.35221171379089355)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3754497468471527)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.34769153594970703)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2884187400341034)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3300345242023468)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.334687739610672)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.2535858154296875)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.5231557488441467)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3676251769065857)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.19400855898857117)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.27886298298835754)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2005366086959839)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.27207672595977783)\n",
            "Training set: Average loss: 0.320265\n",
            "Validation set: Average loss: 0.2960703787245568, Accuracy: 8942/10000 (89.42%)\n",
            "Epoch: 3\n",
            "Training set:0/70000 (0.0 Loss:0.30438393354415894)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2725584805011749)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.19829265773296356)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.3038284480571747)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2579438388347626)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.34382113814353943)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2975953221321106)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2872426211833954)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.31858551502227783)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.238032728433609)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3705573081970215)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.32668575644493103)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.20537732541561127)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3801579475402832)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.21805499494075775)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.25210100412368774)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.4713576138019562)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3549169600009918)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2044079452753067)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.21852408349514008)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.427286833524704)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.32793086767196655)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.1910991668701172)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.318212628364563)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.31026551127433777)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.44975125789642334)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.21522881090641022)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.28714436292648315)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.30878403782844543)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.3354640305042267)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.22896908223628998)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.42213064432144165)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.20341308414936066)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.30175521969795227)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3003302812576294)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.27167052030563354)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2666694223880768)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.18867136538028717)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.3876696527004242)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.13203482329845428)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2363126128911972)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.22197456657886505)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.23515833914279938)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.4424199163913727)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.37180715799331665)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.1944219022989273)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.2523982524871826)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.15896658599376678)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.30113983154296875)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.21259461343288422)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.22029447555541992)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2981470823287964)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.20165793597698212)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.3400537669658661)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.11741140484809875)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.34423279762268066)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.31726670265197754)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.21738293766975403)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2524805963039398)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.36418306827545166)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.4087180495262146)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2965949475765228)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.25286149978637695)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.32484424114227295)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.17109932005405426)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.29735293984413147)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3129929006099701)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.27393725514411926)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.39967453479766846)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.1963801234960556)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.324002742767334)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.1983969807624817)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2394651621580124)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.22745731472969055)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.33512669801712036)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2623291313648224)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.27591294050216675)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.20490694046020508)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.16854068636894226)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.28107860684394836)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2411467283964157)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2304297834634781)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.4254873991012573)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.3527238070964813)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.346112996339798)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.2161288559436798)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.28086942434310913)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.23784199357032776)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.1498124599456787)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2145002782344818)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.16316381096839905)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3553035259246826)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2986481487751007)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.24491974711418152)\n",
            "Training set: Average loss: 0.282948\n",
            "Validation set: Average loss: 0.2740992270172781, Accuracy: 9019/10000 (90.19%)\n",
            "Epoch: 4\n",
            "Training set:0/70000 (0.0 Loss:0.3821764886379242)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.22279421985149384)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.43702834844589233)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.10497800260782242)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14361220598220825)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.17587025463581085)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.296892374753952)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.22323720157146454)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2801525890827179)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.220418319106102)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.2330581694841385)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.27919435501098633)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.35666361451148987)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3005160689353943)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3395909368991852)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.3883410692214966)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.23246127367019653)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.18943023681640625)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.13482151925563812)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.3167705833911896)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.19698214530944824)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.24603401124477386)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.21880917251110077)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.367233544588089)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3162124454975128)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.18717505037784576)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.2471092939376831)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.26852113008499146)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.2895601987838745)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.22632071375846863)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.37165623903274536)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.31511616706848145)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2875453531742096)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2985152304172516)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.24807997047901154)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.25500011444091797)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3726780414581299)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.19321803748607635)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.23407666385173798)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.18710795044898987)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.25773805379867554)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.18596790730953217)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.3057842552661896)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.29758015275001526)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.2816909849643707)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.25852343440055847)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.15886463224887848)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.23806187510490417)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.34799546003341675)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.26034075021743774)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.13218258321285248)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.14349210262298584)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2193111926317215)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2963355779647827)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2904462218284607)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.30683276057243347)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.21832789480686188)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2909066081047058)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.274728924036026)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2989594340324402)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.33793434500694275)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.24155712127685547)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.35148757696151733)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.5431424379348755)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.09852644801139832)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3368552625179291)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2896362841129303)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.40096575021743774)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2093653380870819)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2150416374206543)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.27219951152801514)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.23205721378326416)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.20315766334533691)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.32209131121635437)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.30246004462242126)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3865075707435608)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.32835152745246887)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1889316886663437)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3124370276927948)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3178390860557556)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.196120947599411)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3224528729915619)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.17949718236923218)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.21624888479709625)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.13631248474121094)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.20962795615196228)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3023369014263153)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.12251804769039154)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.24642197787761688)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.21046139299869537)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.18136045336723328)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2804286479949951)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24025675654411316)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.26828479766845703)\n",
            "Training set: Average loss: 0.259737\n",
            "Validation set: Average loss: 0.257959432188113, Accuracy: 9101/10000 (91.01%)\n",
            "Epoch: 5\n",
            "Training set:0/70000 (0.0 Loss:0.2742055654525757)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2290252149105072)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.20107069611549377)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.1764420121908188)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.3292722702026367)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2093839943408966)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.28425467014312744)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.26329419016838074)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2149316817522049)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.18531110882759094)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.16186878085136414)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.23718926310539246)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.3844393491744995)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.2667279541492462)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.28942614793777466)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2348272055387497)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.08972007781267166)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.22085241973400116)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.11443057656288147)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2886274456977844)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.35695788264274597)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.12796650826931)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.18965011835098267)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.3074561357498169)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.28886085748672485)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.21735332906246185)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.4118610918521881)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1575598269701004)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3399149477481842)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.153384268283844)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.1270287185907364)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.22498819231987)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2786688804626465)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.3021700382232666)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.16411522030830383)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.25775107741355896)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.19464945793151855)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.21894948184490204)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2621859908103943)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.15106359124183655)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.12075816839933395)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.18287304043769836)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.49725520610809326)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2233484536409378)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.23975436389446259)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.3299001455307007)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.15650774538516998)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.23150742053985596)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.23698192834854126)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.1529301553964615)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3497166931629181)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2810397148132324)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2293539047241211)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.4213002920150757)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.08715979754924774)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3243800401687622)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.33641013503074646)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.28197312355041504)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.20943213999271393)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.25978460907936096)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.17417405545711517)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.1999943107366562)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.3258349299430847)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.36797037720680237)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.24754096567630768)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.25127094984054565)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3881041407585144)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.28131911158561707)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.20935355126857758)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.24279281497001648)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.2351035624742508)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.2782251238822937)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2907991111278534)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2524910271167755)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.22481445968151093)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.23665763437747955)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.1527581661939621)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1026630774140358)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3330402374267578)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2384367287158966)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.29955360293388367)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.23292753100395203)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.4240776300430298)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1822645217180252)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.4257984161376953)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.29465174674987793)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.16453817486763)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.18334677815437317)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.23655442893505096)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.338548868894577)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.140952467918396)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3158119022846222)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.32220715284347534)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.30493733286857605)\n",
            "Training set: Average loss: 0.242801\n",
            "Validation set: Average loss: 0.248941293496425, Accuracy: 9122/10000 (91.22%)\n",
            "Epoch: 6\n",
            "Training set:0/70000 (0.0 Loss:0.21931128203868866)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2917977571487427)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.28150856494903564)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.18806515634059906)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.20416724681854248)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.16074509918689728)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.19166144728660583)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2990243136882782)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.19185873866081238)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.15830278396606445)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.15144705772399902)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.29380548000335693)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.3787863850593567)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.32401901483535767)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.14542172849178314)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2841462790966034)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2786124050617218)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.14316612482070923)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.08440297096967697)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.3399454355239868)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.13395167887210846)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.13803772628307343)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.2681179344654083)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.17360860109329224)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.2155645787715912)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2338409572839737)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.3833250105381012)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.33049091696739197)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.09458848088979721)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.42831307649612427)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2084691822528839)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.15757249295711517)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.1856800764799118)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2612145245075226)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.17651422321796417)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.20155787467956543)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3214222490787506)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2171698808670044)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.05979861691594124)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.3018602430820465)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2590297758579254)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.1573052853345871)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.3643282353878021)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.14171749353408813)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.12226125597953796)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.25676360726356506)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.25786498188972473)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2213459312915802)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.31495943665504456)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.26901382207870483)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.24764332175254822)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.20879267156124115)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.26882854104042053)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.18249984085559845)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3096209466457367)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2826713025569916)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.19064006209373474)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.24155020713806152)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2348819524049759)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.19538205862045288)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.11278367787599564)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.22787459194660187)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.17845182120800018)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2058367133140564)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.20051521062850952)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.25734156370162964)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.27954912185668945)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.12911531329154968)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3590192198753357)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.21149054169654846)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.31057310104370117)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.16206324100494385)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.11076126992702484)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.29273900389671326)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.32629314064979553)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.18626871705055237)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.07669870555400848)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1129259318113327)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.18197965621948242)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.15367448329925537)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2907131314277649)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.1058315709233284)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2764814496040344)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.3598855435848236)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3458825349807739)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3934025168418884)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2550840675830841)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.22783543169498444)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.27477043867111206)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.15795013308525085)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.1748538464307785)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2257002592086792)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.18608629703521729)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.22046920657157898)\n",
            "Training set: Average loss: 0.228444\n",
            "Validation set: Average loss: 0.24242031290700103, Accuracy: 9157/10000 (91.57%)\n",
            "Epoch: 7\n",
            "Training set:0/70000 (0.0 Loss:0.19923457503318787)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.16703437268733978)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.11421723663806915)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.17581908404827118)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2720196843147278)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.21098139882087708)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.33272725343704224)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.12802234292030334)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.15621359646320343)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1975231021642685)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.12313768267631531)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.1069304347038269)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.18518385291099548)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.4018804132938385)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.17585916817188263)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.13743770122528076)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.19168953597545624)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.27620500326156616)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.28966447710990906)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.12115588784217834)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.3162374496459961)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.15787141025066376)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.2726021707057953)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.17809630930423737)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.128622904419899)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.21065041422843933)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.24961990118026733)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.389957070350647)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.15275636315345764)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.19585773348808289)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2112342119216919)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.21096280217170715)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2210220992565155)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1617559790611267)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.26576828956604004)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.27858006954193115)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2791310250759125)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.32055577635765076)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.28562480211257935)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.22657150030136108)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.27644795179367065)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.260884165763855)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.18521490693092346)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.25784510374069214)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.1807447075843811)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.14433564245700836)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.11391191929578781)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.12491145730018616)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.33975934982299805)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.32563620805740356)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.12209196388721466)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.09764362871646881)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.24695296585559845)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.12787047028541565)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.26869845390319824)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3735140264034271)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.19300779700279236)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.1635076403617859)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.14184021949768066)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.12518061697483063)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3507317900657654)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3482474684715271)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.256708025932312)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.07147015631198883)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.12897400557994843)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.27125343680381775)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2850613594055176)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.19756783545017242)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.15035490691661835)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.09882791340351105)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.22742925584316254)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.11547387391328812)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.16443473100662231)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.28416264057159424)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2505670487880707)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.27632275223731995)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2509780824184418)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.305327832698822)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.09490688890218735)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.23655623197555542)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.23688475787639618)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2459402084350586)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.37292569875717163)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.20162180066108704)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.15836270153522491)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.17215970158576965)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.20815330743789673)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.19167372584342957)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2954917252063751)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.33213871717453003)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.30958855152130127)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.19162750244140625)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2131347954273224)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.15038344264030457)\n",
            "Training set: Average loss: 0.216176\n",
            "Validation set: Average loss: 0.2520904193400957, Accuracy: 9111/10000 (91.11%)\n",
            "Epoch: 8\n",
            "Training set:0/70000 (0.0 Loss:0.11770962923765182)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.14871622622013092)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.13952498137950897)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.22876837849617004)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14128978550434113)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.15631698071956635)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.180502250790596)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.1586858481168747)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.1190885677933693)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3054381012916565)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.16735869646072388)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.22840017080307007)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.22184425592422485)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.16617076098918915)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.12983140349388123)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.14258873462677002)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.23788461089134216)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3266073763370514)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2193383276462555)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.10557200759649277)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.16132961213588715)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.10311999917030334)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.17084647715091705)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.21130268275737762)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.1497897058725357)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.12598161399364471)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.19278743863105774)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1497151106595993)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.1842488795518875)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.23263442516326904)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.31344056129455566)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2603881359100342)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.18239307403564453)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.24184340238571167)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2036811113357544)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.13510403037071228)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.15946198999881744)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2387080043554306)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.07264304906129837)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.2770569920539856)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.20404531061649323)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.1737206131219864)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.11212491244077682)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.1840025782585144)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.17343974113464355)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.195365771651268)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3796975314617157)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.38565778732299805)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.21048158407211304)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.10742221772670746)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.14673636853694916)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.19795705378055573)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.15552918612957)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.08091367781162262)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2865113317966461)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.24371124804019928)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2305920422077179)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.09122572839260101)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.21265850961208344)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.27074962854385376)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.19631174206733704)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.19645489752292633)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.07514189928770065)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.327358216047287)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.12724173069000244)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.16087359189987183)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2772018015384674)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.5094055533409119)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3830960988998413)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.22191759943962097)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.3128661811351776)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.12173425406217575)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.5062482357025146)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.18510502576828003)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2015615701675415)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3434705436229706)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.1510154753923416)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.3660508990287781)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.15150326490402222)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17938289046287537)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.17702904343605042)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2556813359260559)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.25171205401420593)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.5057474970817566)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.15856041014194489)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.28361961245536804)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.12751804292201996)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.15135154128074646)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.21918943524360657)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2168588936328888)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.26702314615249634)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.21352986991405487)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.17973384261131287)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.10044579207897186)\n",
            "Training set: Average loss: 0.209332\n",
            "Validation set: Average loss: 0.23602834991683627, Accuracy: 9148/10000 (91.47999999999999%)\n",
            "Epoch: 9\n",
            "Training set:0/70000 (0.0 Loss:0.10508693754673004)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.19612237811088562)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.2558625042438507)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.27540361881256104)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2691033184528351)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.34897375106811523)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.09540562331676483)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2236969769001007)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.24349163472652435)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.28509214520454407)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.1693817377090454)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.12700052559375763)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.29694992303848267)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.056473203003406525)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.35636967420578003)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.45094436407089233)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.08732674270868301)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.1413833200931549)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.1784825474023819)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.15649022161960602)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.12646304070949554)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.17766830325126648)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.20519354939460754)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.2697427570819855)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.12743957340717316)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.32479405403137207)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.12503711879253387)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.11178284883499146)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.12112049013376236)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.1711277961730957)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.11885040253400803)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.16171102225780487)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.22514206171035767)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2547984719276428)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.15626098215579987)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.16931572556495667)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.12985026836395264)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.1245758980512619)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.18327654898166656)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.1883457452058792)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.18257097899913788)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.11600282788276672)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.12705208361148834)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.36376506090164185)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.11194679886102676)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.22713997960090637)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.14177939295768738)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.13423918187618256)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.11074355989694595)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.17619584500789642)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.12118282914161682)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.23377548158168793)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.10997022688388824)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.1630845069885254)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.16399376094341278)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.322504460811615)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2887478470802307)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.12502999603748322)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.21531786024570465)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.33187437057495117)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.12110646069049835)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.19154267013072968)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.16763903200626373)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.1754002422094345)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.10988397151231766)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.1157710999250412)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.14492279291152954)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.15660826861858368)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.11960875988006592)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.17901268601417542)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.1993142068386078)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.12311237305402756)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.1746690273284912)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.1666785329580307)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.12770314514636993)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2725110650062561)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.18039308488368988)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.13726958632469177)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.12652423977851868)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.11453860253095627)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.11181104183197021)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.1505369395017624)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.15291887521743774)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1856957972049713)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.14651799201965332)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.18715780973434448)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.21875014901161194)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.2808246314525604)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.1542784720659256)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.1720295548439026)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.16277042031288147)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.13106709718704224)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.26667100191116333)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.13086120784282684)\n",
            "Training set: Average loss: 0.200124\n",
            "Validation set: Average loss: 0.247873506253692, Accuracy: 9125/10000 (91.25%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.12569615244865417)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.10893060266971588)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.2630160450935364)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.13635265827178955)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.12921616435050964)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.23159945011138916)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.16142243146896362)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.17914073169231415)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.21294057369232178)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2490006387233734)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.2461208999156952)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.0768703818321228)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.20519492030143738)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.0732305571436882)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.20416486263275146)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.14864949882030487)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.16030512750148773)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.25776514410972595)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.10278742015361786)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2547009289264679)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.10732851177453995)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.08566709607839584)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.0482388436794281)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.27736523747444153)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.17642895877361298)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.16142047941684723)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.14374537765979767)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.2395525425672531)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.15484373271465302)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.182967409491539)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.17448455095291138)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.31520894169807434)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.10416968166828156)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.14110980927944183)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.19599933922290802)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.17122741043567657)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.27209559082984924)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.17793914675712585)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.1335047334432602)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.23517625033855438)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.12221866101026535)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.16127033531665802)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.35601410269737244)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.15781578421592712)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.12118496745824814)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.20358362793922424)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.13241322338581085)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2703963816165924)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.26541224122047424)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.39594313502311707)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.1245754286646843)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.25304684042930603)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.18560661375522614)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.07751988619565964)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2092587649822235)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.19000869989395142)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.23247011005878448)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.1497155725955963)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.09644831717014313)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.204724982380867)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.13131625950336456)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2312430739402771)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2982194423675537)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.31297460198402405)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.3331063985824585)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.1570926308631897)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.14981529116630554)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.20193351805210114)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.07601119577884674)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.3925721347332001)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.11453012377023697)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.05520488694310188)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2206364870071411)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2259046882390976)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.10779242217540741)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.11320329457521439)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.12824854254722595)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.15683874487876892)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.2188776135444641)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1209993064403534)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.18811661005020142)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.1972080022096634)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.13020266592502594)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.10450995713472366)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.33239468932151794)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3088405430316925)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.15313178300857544)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.11529139429330826)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.28855466842651367)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3329308032989502)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.13640381395816803)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.11852385848760605)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2272404134273529)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.3677375614643097)\n",
            "Training set: Average loss: 0.194609\n",
            "Validation set: Average loss: 0.238690826638489, Accuracy: 9157/10000 (91.57%)\n",
            "Accuracy for fold 4: 91.57%\n",
            "Fold 5/7\n",
            "Epoch: 1\n",
            "Training set:0/70000 (0.0 Loss:2.2990872859954834)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:1.3342280387878418)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.7714967727661133)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.844436526298523)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.6962966322898865)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.6723580360412598)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.6598710417747498)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.5944832563400269)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.5853503942489624)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.5520601868629456)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.6223096251487732)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.3928855061531067)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.7496153712272644)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.4879646301269531)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.5695177912712097)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.6164120435714722)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.5611437559127808)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.46973294019699097)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.5563178658485413)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.3971240520477295)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.5553957223892212)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.5387681126594543)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3859635889530182)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.6786981225013733)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.5375680923461914)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.4163244068622589)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.44545602798461914)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.5303813219070435)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.48134875297546387)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.5513021349906921)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2788711190223694)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.4765965938568115)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.5311560034751892)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.48139718174934387)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.6880220770835876)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.496685653924942)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.32938647270202637)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.4999242424964905)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.45208945870399475)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.30899158120155334)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.4994942545890808)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.34785377979278564)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.34174394607543945)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.4321608245372772)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.3217792510986328)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.23010766506195068)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.36976271867752075)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.38288459181785583)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3324623703956604)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.3831408619880676)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.4543376863002777)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.28727173805236816)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.43515217304229736)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.3489261567592621)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3765338361263275)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.5788238048553467)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.3212760388851166)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.31765198707580566)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.44706088304519653)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3719819486141205)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.45906588435173035)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2938261032104492)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.47296950221061707)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.24126382172107697)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.5605873465538025)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.2745800018310547)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.30819830298423767)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.25115758180618286)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.24606385827064514)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.6625776290893555)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4882168471813202)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3333020508289337)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3831632435321808)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.36074838042259216)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.5598697066307068)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.28761062026023865)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.30953195691108704)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.3448678255081177)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19832265377044678)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.5754455327987671)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.3902399241924286)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.506352424621582)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.27385663986206055)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.35051074624061584)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2560500204563141)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.5116782784461975)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.49151545763015747)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.3251628279685974)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.38789185881614685)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3850253224372864)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.36017560958862305)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.37725022435188293)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.16422122716903687)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.19227217137813568)\n",
            "Training set: Average loss: 0.475749\n",
            "Validation set: Average loss: 0.3339391170413631, Accuracy: 8804/10000 (88.03999999999999%)\n",
            "Epoch: 2\n",
            "Training set:0/70000 (0.0 Loss:0.44001254439353943)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.241620272397995)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.35887059569358826)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.30992379784584045)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.43916651606559753)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2272602915763855)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.29205217957496643)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2364157736301422)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2946835160255432)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.23230226337909698)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.2596417963504791)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.37574976682662964)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.51529461145401)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3535346984863281)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.446646124124527)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.3391115367412567)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.3235030770301819)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.5312941074371338)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.31387490034103394)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.4124937057495117)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.31836003065109253)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.27310726046562195)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.4401887357234955)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.31510618329048157)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3074939250946045)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.28916338086128235)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.35650336742401123)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.24449409544467926)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.34916311502456665)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.28518474102020264)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2667667269706726)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.24445736408233643)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.5110993981361389)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.5125647187232971)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3216246962547302)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.5218672752380371)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2021845281124115)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3643180727958679)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.1543111950159073)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.3016909956932068)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2656017243862152)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.2934771478176117)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.26618531346321106)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.240280419588089)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.37882155179977417)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.35178738832473755)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3332473337650299)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.36808761954307556)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.4020914137363434)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2745703458786011)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3766818046569824)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.39637941122055054)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.3238696753978729)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.34542202949523926)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3160971701145172)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3573687672615051)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.28618451952934265)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2779928743839264)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.3834461271762848)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3395528793334961)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.33443722128868103)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.301581472158432)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.29668766260147095)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.35245612263679504)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.46483758091926575)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.189710333943367)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.34725847840309143)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.3316509425640106)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.5385922789573669)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.3631022274494171)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.373445063829422)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.320588082075119)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.41004201769828796)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2850000858306885)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.23549945652484894)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.23255427181720734)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2842247784137726)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.5794786810874939)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.415160596370697)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.4494117200374603)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.4056396484375)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.294657438993454)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3258448839187622)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.37096408009529114)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3515137732028961)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.2938007414340973)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.2502478361129761)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.3861411213874817)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.1339273452758789)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2745903730392456)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.1827823966741562)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.27274730801582336)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.16195623576641083)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2686484456062317)\n",
            "Training set: Average loss: 0.334816\n",
            "Validation set: Average loss: 0.2873399267151098, Accuracy: 8978/10000 (89.78%)\n",
            "Epoch: 3\n",
            "Training set:0/70000 (0.0 Loss:0.3321945071220398)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.22902324795722961)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.30930593609809875)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.23357801139354706)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.21558432281017303)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.19924724102020264)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2750515043735504)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.40608763694763184)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2612311840057373)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2931084930896759)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.31294915080070496)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.23957540094852448)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2355714738368988)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.16549751162528992)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.22669439017772675)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.42436841130256653)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.22777171432971954)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3387557566165924)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.3215550482273102)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.207464337348938)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.26524877548217773)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.19388283789157867)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3367576003074646)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.39949777722358704)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.26222994923591614)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.19720444083213806)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.40574464201927185)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.19515126943588257)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.14096349477767944)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.25673767924308777)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.26020267605781555)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.31261155009269714)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2782976031303406)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2647320628166199)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.22751295566558838)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.25553372502326965)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3016407787799835)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.5318151712417603)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.18375803530216217)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.30417484045028687)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.27913227677345276)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.14489994943141937)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.24522455036640167)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.30053016543388367)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.35073134303092957)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.45181554555892944)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.20689135789871216)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.18777458369731903)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.20169900357723236)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.35701435804367065)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3019067645072937)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.22435380518436432)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.4398448169231415)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.321552038192749)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3558197617530823)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.29469090700149536)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.28194472193717957)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2312874048948288)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2334735244512558)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3077121376991272)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.397552490234375)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.26115021109580994)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.32349929213523865)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.269292950630188)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.3558482229709625)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3409574031829834)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3712967038154602)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.4684670865535736)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.29464083909988403)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.18319357931613922)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.29796895384788513)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.31296566128730774)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.30875858664512634)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.37617507576942444)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.3150720000267029)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.24737755954265594)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.21034376323223114)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2053411453962326)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.1713041067123413)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.45030683279037476)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.28933286666870117)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.1887439340353012)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.25901323556900024)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.35961490869522095)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3405371904373169)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.40892067551612854)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.4225492477416992)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.4028315544128418)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.22150272130966187)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.35932984948158264)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.144061878323555)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2871778607368469)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.3640121519565582)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.3023746907711029)\n",
            "Training set: Average loss: 0.297708\n",
            "Validation set: Average loss: 0.284705947965953, Accuracy: 9006/10000 (90.06%)\n",
            "Epoch: 4\n",
            "Training set:0/70000 (0.0 Loss:0.19722916185855865)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.5114452242851257)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.23875737190246582)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.16054189205169678)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.22136977314949036)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.1595911830663681)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.23148949444293976)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2896644175052643)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.30038395524024963)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.30782005190849304)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.22309722006320953)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.21593889594078064)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.19544850289821625)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.4289828836917877)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.29644426703453064)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.22479932010173798)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.12527666985988617)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3693833351135254)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.35340961813926697)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.230216383934021)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.188166081905365)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.34848979115486145)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.351286381483078)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.28792324662208557)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.36273086071014404)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.3358075022697449)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.16316570341587067)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.3154849708080292)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.4142869710922241)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.2989214360713959)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.3540371060371399)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.31645867228507996)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2484786957502365)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1112535297870636)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.29341962933540344)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.3876078724861145)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.14396341145038605)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2226056009531021)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.4125099778175354)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.33419492840766907)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2127346694469452)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3042621314525604)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.34181898832321167)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.1933150291442871)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.31890803575515747)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.2936924993991852)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3692530393600464)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.31590738892555237)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.30533885955810547)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.23253461718559265)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.336540162563324)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2520993649959564)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.18884627521038055)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.4684171676635742)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.19185979664325714)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3434832990169525)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2741619050502777)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.44345396757125854)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.18758352100849152)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.4182506501674652)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.31139764189720154)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3358761668205261)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.35984721779823303)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.35731369256973267)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.212543785572052)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.30155235528945923)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3891749978065491)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.29458269476890564)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.37145721912384033)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.23165452480316162)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.36291205883026123)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3055196702480316)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.34955447912216187)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.41428324580192566)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2816215753555298)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.15524502098560333)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.24875982105731964)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2960347533226013)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.37051525712013245)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3397170305252075)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2154158353805542)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.19788825511932373)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.29723793268203735)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.2138979434967041)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3027946650981903)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.10358423739671707)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.28931522369384766)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.18656805157661438)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.28645628690719604)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2674621343612671)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.35997268557548523)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3986539840698242)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.21477697789669037)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2933206558227539)\n",
            "Training set: Average loss: 0.273862\n",
            "Validation set: Average loss: 0.2506039680758859, Accuracy: 9087/10000 (90.86999999999999%)\n",
            "Epoch: 5\n",
            "Training set:0/70000 (0.0 Loss:0.17477604746818542)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2581702470779419)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.35737162828445435)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.21332412958145142)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14233359694480896)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.26076602935791016)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2090606540441513)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.20518691837787628)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.26817795634269714)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1788434386253357)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.4116117060184479)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.17413866519927979)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.23829330503940582)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.25882863998413086)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.28394559025764465)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.3224142789840698)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.29352787137031555)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.27213770151138306)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2943492531776428)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.17329487204551697)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.19354835152626038)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.24479582905769348)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.23824314773082733)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.45295488834381104)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.34020549058914185)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.20692449808120728)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.20217354595661163)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.21081005036830902)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.23249821364879608)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.25169843435287476)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.24983470141887665)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.34125256538391113)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2508776783943176)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.443207710981369)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2704225778579712)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.2880377471446991)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.35889288783073425)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2161634862422943)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2539701759815216)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.383698046207428)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2524896264076233)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3357129395008087)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.19995568692684174)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.36136698722839355)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.16936124861240387)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.43039605021476746)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.34310126304626465)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.3949712812900543)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.18027429282665253)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.3133595287799835)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.33513760566711426)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.32447245717048645)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.13197878003120422)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2188197374343872)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2098967581987381)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3635157346725464)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.1996934413909912)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.25043296813964844)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.29790541529655457)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.30065587162971497)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.23972617089748383)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.19028525054454803)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.18214364349842072)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.19835533201694489)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2400849461555481)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.22099952399730682)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.09724418818950653)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.3026992380619049)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.33781927824020386)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2621491253376007)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4761797785758972)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.21739162504673004)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.27317795157432556)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.21311236917972565)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.16244888305664062)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3772837817668915)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.21916502714157104)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.19497248530387878)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.17785699665546417)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3221242129802704)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.27353644371032715)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.32330018281936646)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.21310274302959442)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.40775325894355774)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.19232366979122162)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.19931280612945557)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.16956277191638947)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.18922778964042664)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2383849322795868)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.21163465082645416)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.2762766480445862)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.20496055483818054)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24288709461688995)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.22681649029254913)\n",
            "Training set: Average loss: 0.254713\n",
            "Validation set: Average loss: 0.24633249849270863, Accuracy: 9141/10000 (91.41%)\n",
            "Epoch: 6\n",
            "Training set:0/70000 (0.0 Loss:0.25721991062164307)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.30535316467285156)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.18702548742294312)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.20641909539699554)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2375609427690506)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.20706601440906525)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.17378252744674683)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.11627921462059021)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.2978171706199646)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.24781696498394012)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.12572382390499115)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2996600866317749)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.4742913544178009)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.14212103188037872)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.22882303595542908)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.23224808275699615)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.1382666379213333)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.28812387585639954)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.21725739538669586)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2522026598453522)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.18491074442863464)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.2360379695892334)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.1633538156747818)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.1209125891327858)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.36408114433288574)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.23443865776062012)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.21051888167858124)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.3223341405391693)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.32867392897605896)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.25659382343292236)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2712160348892212)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.1309157758951187)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2674463391304016)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.17254239320755005)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2044890820980072)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.4355068802833557)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2403709888458252)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.20261366665363312)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.36447039246559143)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.21603944897651672)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.32346397638320923)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.40758100152015686)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.1921856850385666)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.16367881000041962)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.14783459901809692)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.17781800031661987)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.17645281553268433)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.34436994791030884)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.24293752014636993)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.309128999710083)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.2622007131576538)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.24871398508548737)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.29131200909614563)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.20588457584381104)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.13032567501068115)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.22586242854595184)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2822941541671753)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.3567642271518707)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.21406063437461853)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.37430813908576965)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.4358716905117035)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2696133852005005)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.18670359253883362)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.19330769777297974)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2615877091884613)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.1984303891658783)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.14711511135101318)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.2580733001232147)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.2092403918504715)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.38153356313705444)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.16123493015766144)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.20892925560474396)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.18670444190502167)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.31684359908103943)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.32110148668289185)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.12473537027835846)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.30003905296325684)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.18338188529014587)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.33696091175079346)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3121497929096222)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.29877740144729614)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.16557511687278748)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.23004934191703796)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.23077036440372467)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.27250993251800537)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.4926985800266266)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.11723485589027405)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.304228812456131)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.3704589009284973)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.39925122261047363)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.09479504078626633)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.21255448460578918)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.3949544429779053)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.19732028245925903)\n",
            "Training set: Average loss: 0.241480\n",
            "Validation set: Average loss: 0.23740029482135347, Accuracy: 9154/10000 (91.53999999999999%)\n",
            "Epoch: 7\n",
            "Training set:0/70000 (0.0 Loss:0.15798965096473694)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.20774000883102417)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.19587823748588562)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.20138832926750183)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.32900893688201904)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2584891617298126)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.20794717967510223)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.34369799494743347)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.1389472782611847)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.15355169773101807)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.28551536798477173)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.09469906240701675)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.1404513716697693)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.14597906172275543)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.1864825040102005)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.26009300351142883)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.11679232865571976)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.09164085239171982)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.1298176497220993)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.14529265463352203)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.10313917696475983)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.16603155434131622)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.2519991397857666)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.15242035686969757)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.21383479237556458)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.19715212285518646)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.0740293562412262)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.18404628336429596)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.20707815885543823)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.3220991790294647)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2943073809146881)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.19775083661079407)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.15699981153011322)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2717585861682892)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.297689825296402)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.10382924228906631)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.36701640486717224)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.36110275983810425)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.24774464964866638)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.11127611249685287)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.11075425148010254)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.394228994846344)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.40000173449516296)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.26791054010391235)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.20260731875896454)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.12076136469841003)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.1178453117609024)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.11486276239156723)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2655542492866516)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.3871534466743469)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.15676993131637573)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.33040651679039)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.24852795898914337)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.27820509672164917)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.17214035987854004)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.21410490572452545)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.1696559339761734)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.142727330327034)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.22151686251163483)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.1751347929239273)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.24156829714775085)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2384229600429535)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.08490021526813507)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.09542565792798996)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2596673369407654)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.25318270921707153)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.41576799750328064)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.24801313877105713)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3031628131866455)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.21891260147094727)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.4059073328971863)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3166949152946472)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.1610220968723297)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20206208527088165)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.16040141880512238)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.20827801525592804)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.14664806425571442)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1443253606557846)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.098476342856884)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2639092206954956)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.10765949636697769)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.27986830472946167)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1748461276292801)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.31377580761909485)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.46810975670814514)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3092629313468933)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.21217967569828033)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.2612941563129425)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.1780555099248886)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.13040843605995178)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.254773885011673)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.15088042616844177)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.15969617664813995)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.15966244041919708)\n",
            "Training set: Average loss: 0.228282\n",
            "Validation set: Average loss: 0.2309636853777679, Accuracy: 9156/10000 (91.56%)\n",
            "Epoch: 8\n",
            "Training set:0/70000 (0.0 Loss:0.4613601863384247)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.20317307114601135)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.3446408212184906)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.10971666872501373)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.14740416407585144)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2267051488161087)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.14078617095947266)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.4353541135787964)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.17574052512645721)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.14918582141399384)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3020564019680023)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.13743141293525696)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.23083429038524628)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.282851904630661)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.14078567922115326)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2339443862438202)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.3216652572154999)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.42083632946014404)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.23951935768127441)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.16182903945446014)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.13070906698703766)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.19499827921390533)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.16682153940200806)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.15772350132465363)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.12217170745134354)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2502017617225647)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.3086743652820587)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.2246568650007248)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.10219036042690277)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.20935149490833282)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.1734474003314972)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.45119330286979675)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.30650001764297485)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2937270700931549)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3180505037307739)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.28459975123405457)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2587689757347107)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.324567973613739)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.16529174149036407)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.17520368099212646)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.27433115243911743)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.1919325888156891)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.21189212799072266)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.14558428525924683)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.2584837079048157)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.19087746739387512)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.1514694094657898)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2737823724746704)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2627090513706207)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.14281371235847473)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3282313346862793)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.06378627568483353)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2017260491847992)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.10990282148122787)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2223527878522873)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.349090576171875)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.28719738125801086)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.19458447396755219)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2367548644542694)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.24541215598583221)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.22232592105865479)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.23345895111560822)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.20644046366214752)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.13455869257450104)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.3199625313282013)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.2916882336139679)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.21032670140266418)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.21057012677192688)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.07872090488672256)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.2686401903629303)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.2630535960197449)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.45043662190437317)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.12751992046833038)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20553360879421234)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.28289979696273804)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3350491225719452)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.12643550336360931)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.18089646100997925)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.30588752031326294)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.19669900834560394)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.18685969710350037)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3513775169849396)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2598613500595093)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.13670805096626282)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2803642749786377)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.2059251219034195)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.10230938345193863)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.23646265268325806)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.22603395581245422)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.30437618494033813)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.18989057838916779)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.1698196828365326)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2688200771808624)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.18921473622322083)\n",
            "Training set: Average loss: 0.218518\n",
            "Validation set: Average loss: 0.23028417009931462, Accuracy: 9194/10000 (91.94%)\n",
            "Epoch: 9\n",
            "Training set:0/70000 (0.0 Loss:0.13272427022457123)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.21701689064502716)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.1135924756526947)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.263121634721756)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.17521002888679504)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.15643756091594696)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2847246825695038)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.17480121552944183)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.15316954255104065)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.22377751767635345)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.28154313564300537)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.14448735117912292)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.25873222947120667)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.1833002269268036)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.15555766224861145)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.23384499549865723)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.15051668882369995)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.10142838209867477)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.33850914239883423)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.12864485383033752)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.1516956090927124)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3099718391895294)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.29241031408309937)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.3613171875476837)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.4440213143825531)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2632514238357544)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.36233916878700256)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.11334437131881714)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.2614196240901947)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.1674286276102066)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.24514001607894897)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.18437498807907104)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.3277686834335327)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.14888855814933777)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.14108231663703918)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.11225217580795288)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.4072417616844177)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.1656888723373413)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.1758313775062561)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.1863924264907837)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.1670549213886261)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.26782965660095215)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.2985343933105469)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.1757582575082779)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.1887756884098053)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.2317821979522705)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.23220549523830414)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.1393868625164032)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.15198081731796265)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.14641334116458893)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.14270050823688507)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2485836297273636)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2828872501850128)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.15802407264709473)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.28400254249572754)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.36870279908180237)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.20628467202186584)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.445720374584198)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.14971843361854553)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.12675845623016357)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.29375842213630676)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3406900465488434)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.16138385236263275)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.31119102239608765)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.18832653760910034)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3318382203578949)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.23912520706653595)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.17736804485321045)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.19081756472587585)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.25056761503219604)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.1123509332537651)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3092237412929535)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.18418800830841064)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.49124768376350403)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.22066238522529602)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.17190109193325043)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.29778823256492615)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.15266729891300201)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.21771901845932007)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.24923205375671387)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.3246271312236786)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.27027788758277893)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2127646505832672)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.10075536370277405)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.26373615860939026)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.05436916649341583)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.08197612315416336)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.33576270937919617)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.1434917151927948)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.16729289293289185)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.14607572555541992)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2144538015127182)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.23751024901866913)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.25682932138442993)\n",
            "Training set: Average loss: 0.209688\n",
            "Validation set: Average loss: 0.23722332791917644, Accuracy: 9136/10000 (91.36%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.3131532669067383)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.1042298674583435)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.07890312373638153)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.21687354147434235)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.20158225297927856)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.17308826744556427)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.33452799916267395)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.3247208297252655)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.13923975825309753)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2163335382938385)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.20227894186973572)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.20243309438228607)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.1710153967142105)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.1781151443719864)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.24781964719295502)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.33667847514152527)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2503100335597992)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.14530016481876373)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.22521501779556274)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.07498577982187271)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.21781662106513977)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.1690526306629181)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.228506401181221)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.24782079458236694)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.20151011645793915)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.13379740715026855)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.201787531375885)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1840371936559677)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.12876653671264648)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.05374559760093689)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.15415199100971222)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2901128828525543)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.07052828371524811)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1400286704301834)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2793186902999878)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.26508045196533203)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1834964156150818)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.19485943019390106)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2348310500383377)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.1893390566110611)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.13440930843353271)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.23584644496440887)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.20815235376358032)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.06435602903366089)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.16505634784698486)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.13629813492298126)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.18858663737773895)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.18031324446201324)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.18327732384204865)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.1603020429611206)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.294258713722229)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.0992678552865982)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2972773611545563)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.1534689962863922)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.1198895126581192)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.16265393793582916)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.280192106962204)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.11786846071481705)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.07194765657186508)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.31231144070625305)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.21419429779052734)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3485032618045807)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.3307943344116211)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.31253743171691895)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2676909565925598)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.08612214773893356)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.24132756888866425)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.36511364579200745)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.17696720361709595)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.20742088556289673)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.19612614810466766)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.21360348165035248)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.25361114740371704)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.2150636613368988)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2082238644361496)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.168579563498497)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.09975220263004303)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2571510970592499)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.13930325210094452)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.31875473260879517)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2367502897977829)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2794038951396942)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1125720888376236)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.18305709958076477)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.14681054651737213)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3790917992591858)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.19170217216014862)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.2648656368255615)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.41464099287986755)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.27592894434928894)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.43296268582344055)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3617951571941376)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.27985039353370667)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1382727026939392)\n",
            "Training set: Average loss: 0.202630\n",
            "Validation set: Average loss: 0.22023322456961225, Accuracy: 9209/10000 (92.09%)\n",
            "Accuracy for fold 5: 92.09%\n",
            "Fold 6/7\n",
            "Epoch: 1\n",
            "Training set:0/70000 (0.0 Loss:2.30570125579834)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:1.491777777671814)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.9259088635444641)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.933638334274292)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.741912305355072)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.8965214490890503)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.575298547744751)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.7198194265365601)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.7768846750259399)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.6811345815658569)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.49519604444503784)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.5317284464836121)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.6470433473587036)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.4967538118362427)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.5683122873306274)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.5716856122016907)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.5232528448104858)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.3828321695327759)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.3995489776134491)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.6763322353363037)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.6177671551704407)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3872147798538208)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.5593212842941284)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.46382761001586914)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.5095738172531128)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.3067629337310791)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.4518856108188629)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.5209845900535583)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.42206934094429016)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.6850994229316711)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.5918124318122864)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.3279225826263428)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.387186735868454)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.46964535117149353)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.4263738989830017)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.31707197427749634)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3453734219074249)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.46466124057769775)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.4777236580848694)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.38664740324020386)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.3713323473930359)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.4916844367980957)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.38076260685920715)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.4470846354961395)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.6502518653869629)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.3208322525024414)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.687470018863678)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.5277060270309448)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.35956358909606934)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.3982275128364563)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.38573727011680603)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.29407936334609985)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.49106141924858093)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.4802822172641754)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.34294408559799194)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.25631183385849)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.349618136882782)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.40041157603263855)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.4466383755207062)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3273375630378723)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.47669166326522827)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.385077565908432)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.20665764808654785)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.15907903015613556)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.329193115234375)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3984735310077667)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.5065663456916809)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.4725576341152191)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3361336290836334)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.4699614942073822)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.5239754915237427)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3327292799949646)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3945951759815216)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.33108723163604736)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.40971317887306213)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.26861295104026794)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.4805319309234619)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.355341374874115)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.21129338443279266)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.260663241147995)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.4667520225048065)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.24519185721874237)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3573487401008606)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.33837929368019104)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2002185881137848)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.34660786390304565)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.5345323085784912)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.5559172630310059)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.32067129015922546)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.43107810616493225)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.43851521611213684)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.5005179643630981)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.16497007012367249)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.42329394817352295)\n",
            "Training set: Average loss: 0.468714\n",
            "Validation set: Average loss: 0.33979024448592193, Accuracy: 8784/10000 (87.83999999999999%)\n",
            "Epoch: 2\n",
            "Training set:0/70000 (0.0 Loss:0.3481229543685913)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.48463135957717896)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.2846049666404724)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.4083958864212036)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.27630940079689026)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.37839287519454956)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.32449623942375183)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.31931072473526)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.32879963517189026)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.40351536870002747)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3555998206138611)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.21689464151859283)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.30792686343193054)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.6103950142860413)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.39271286129951477)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.37066012620925903)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.21982650458812714)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.5280987024307251)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.26438355445861816)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.3462791442871094)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.14085057377815247)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.2874581217765808)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.5477831363677979)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.2742481827735901)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.36425742506980896)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.5474830865859985)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.47300511598587036)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.25866198539733887)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.26648324728012085)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.24488112330436707)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.4205706715583801)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.4728063642978668)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.5361219644546509)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.22263212502002716)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.42650550603866577)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.33973535895347595)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.36969470977783203)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3223544955253601)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.23157580196857452)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.38049328327178955)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.23364193737506866)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3114939033985138)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.4553058445453644)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.3721655309200287)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.35539892315864563)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.40439411997795105)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3437879979610443)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.33049288392066956)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.259787917137146)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.4032664895057678)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3146176338195801)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.38727524876594543)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.3900110721588135)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.34161585569381714)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3172019124031067)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.18362216651439667)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.3637663424015045)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.3288653790950775)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2629428505897522)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.20655480027198792)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3316739797592163)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.2701590061187744)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.33787229657173157)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.3057168424129486)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.4058131277561188)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.29710713028907776)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2003912478685379)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.40696045756340027)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.22764372825622559)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.35158219933509827)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.1708649843931198)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3194703161716461)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.31937578320503235)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.3111723065376282)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.31271347403526306)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.32850122451782227)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.33033639192581177)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.25951364636421204)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3370899558067322)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.37751516699790955)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2910322844982147)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.35399749875068665)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2966539263725281)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.30519339442253113)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3234274983406067)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.4333663880825043)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3616776764392853)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.28338780999183655)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.26980307698249817)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.318072646856308)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.36965611577033997)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2613886594772339)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.27816739678382874)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2165422886610031)\n",
            "Training set: Average loss: 0.335969\n",
            "Validation set: Average loss: 0.29470009233351724, Accuracy: 8944/10000 (89.44%)\n",
            "Epoch: 3\n",
            "Training set:0/70000 (0.0 Loss:0.2594429552555084)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.4764311611652374)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.25734594464302063)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.5433204770088196)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.37046411633491516)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.31645965576171875)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.3618433475494385)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.19980086386203766)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.307463675737381)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3186068534851074)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.2622615396976471)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.35779380798339844)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.30638790130615234)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.4661894142627716)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.19444170594215393)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.4587286114692688)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.1419343650341034)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2633184492588043)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.4230066239833832)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.26204007863998413)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.18696202337741852)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.2615380585193634)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.43201953172683716)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.22407430410385132)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.22949054837226868)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.28822094202041626)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.22869303822517395)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.2756992280483246)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.40495067834854126)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.19995559751987457)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.25349393486976624)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.4542141258716583)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.24349449574947357)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.47911372780799866)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.36087167263031006)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.23714813590049744)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1998060792684555)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.23621626198291779)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.3006005883216858)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.4046972393989563)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.25562724471092224)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.33887818455696106)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.45113405585289)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.39961129426956177)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.3093821406364441)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.29937300086021423)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.23811399936676025)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.3166821599006653)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.28629085421562195)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.4792545735836029)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.38150715827941895)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.39556244015693665)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.32753247022628784)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.396588534116745)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.6180397868156433)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.3099208474159241)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.24154244363307953)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2487955242395401)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.37980738282203674)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.20607693493366241)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3012218773365021)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.38636934757232666)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.12155769765377045)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2824881672859192)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.17016679048538208)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3362797498703003)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3324865996837616)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.34428325295448303)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.32611793279647827)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.385815292596817)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.2729789614677429)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.33933934569358826)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3318570554256439)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.19343160092830658)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.14244559407234192)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.4006032645702362)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2884722650051117)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.3124678134918213)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3082242012023926)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1664215326309204)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.19258807599544525)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.28435540199279785)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.22208836674690247)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.21144245564937592)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.3052555024623871)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.37106359004974365)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.22339347004890442)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.1820639967918396)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.38176631927490234)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2716798186302185)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.30145394802093506)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3015329837799072)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24081045389175415)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.16952499747276306)\n",
            "Training set: Average loss: 0.294719\n",
            "Validation set: Average loss: 0.2764054115410823, Accuracy: 9025/10000 (90.25%)\n",
            "Epoch: 4\n",
            "Training set:0/70000 (0.0 Loss:0.3303544223308563)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2575780153274536)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.24400557577610016)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.28291264176368713)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.1861935406923294)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.1654033362865448)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2726919651031494)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.1751469522714615)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.26401636004447937)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3292050361633301)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.1961980015039444)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.3165217936038971)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.22547954320907593)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.14428219199180603)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.271946519613266)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.24770328402519226)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.21055686473846436)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.22945676743984222)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.23971308767795563)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2607218623161316)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.13345929980278015)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.26170867681503296)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.22457681596279144)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.21558794379234314)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3479568362236023)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.303865909576416)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.30368772149086)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.08778833597898483)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.1742817908525467)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.21505923569202423)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.21278171241283417)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.24661530554294586)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.24793045222759247)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1942354142665863)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3572475016117096)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.3095446825027466)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1429460644721985)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2612808048725128)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2290380299091339)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.24794214963912964)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.1628449410200119)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.30017632246017456)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.34405088424682617)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.32485082745552063)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.3386525809764862)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.2481834590435028)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3339873254299164)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.24412119388580322)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.34859830141067505)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.22292543947696686)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.20678849518299103)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.3545883893966675)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.32682064175605774)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.30526766180992126)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.16862529516220093)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2554721534252167)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.19200189411640167)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.42136678099632263)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.18675583600997925)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.19977392256259918)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.19730457663536072)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.4981372654438019)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.25129660964012146)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2993756830692291)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.3989335298538208)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.15948490798473358)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3292226791381836)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.2985422611236572)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.4043184220790863)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.4211522042751312)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.3115939199924469)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.1327681541442871)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3269369304180145)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20324614644050598)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.35799217224121094)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.23378300666809082)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.3154909014701843)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.12162205576896667)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.2026691883802414)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.18494147062301636)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.49028873443603516)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.21583525836467743)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.4721931517124176)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.08176076412200928)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.23987525701522827)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3386699855327606)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.22708602249622345)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.15899766981601715)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2135433554649353)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.24159462749958038)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.40942609310150146)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.3000568151473999)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.36283376812934875)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1527153104543686)\n",
            "Training set: Average loss: 0.270078\n",
            "Validation set: Average loss: 0.2500259089431945, Accuracy: 9091/10000 (90.91%)\n",
            "Epoch: 5\n",
            "Training set:0/70000 (0.0 Loss:0.18156620860099792)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.18882353603839874)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.19307702779769897)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.25991660356521606)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.10468726605176926)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2545750141143799)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.26797348260879517)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.2946266829967499)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.20425714552402496)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.10119029879570007)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.1823156625032425)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2773425877094269)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.40728455781936646)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.20977433025836945)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.22238394618034363)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.246347114443779)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2759529650211334)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.19927050173282623)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.17554627358913422)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.19645048677921295)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.21344751119613647)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.17898106575012207)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.32237061858177185)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.39627552032470703)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.19400496780872345)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2778969705104828)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.1698819249868393)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.16678790748119354)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.3383537232875824)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.35404735803604126)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.3351418077945709)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.1703938990831375)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.17597776651382446)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.19378815591335297)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2246091663837433)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.28015026450157166)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.0990782305598259)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.1527896225452423)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.42970308661460876)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.26149433851242065)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.16802260279655457)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.2091185748577118)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.18130287528038025)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2057400494813919)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.16886480152606964)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.3277031183242798)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.2099955826997757)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2084282487630844)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.11162491142749786)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.32108497619628906)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.23058457672595978)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.409036248922348)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.19869600236415863)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.21135422587394714)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3034556210041046)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.22214443981647491)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.3086705505847931)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.20971645414829254)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2797399163246155)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2836475968360901)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.20965714752674103)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.27696648240089417)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.25888144969940186)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.20963822305202484)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.15826325118541718)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.17279839515686035)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.16667304933071136)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.1959013044834137)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3161446154117584)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.1477952003479004)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.0878017246723175)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.24571406841278076)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3739423155784607)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.21619121730327606)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.21771323680877686)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.37297502160072327)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.14146625995635986)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.19329947233200073)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.11521083861589432)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.3449115753173828)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.1492258608341217)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2386898696422577)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.324693888425827)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.3151663541793823)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2153157889842987)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3350693881511688)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.17238177359104156)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.1445426642894745)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.29114487767219543)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.18625354766845703)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.37732183933258057)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.12155898660421371)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2131325751543045)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1733108013868332)\n",
            "Training set: Average loss: 0.250421\n",
            "Validation set: Average loss: 0.24849802450199796, Accuracy: 9089/10000 (90.89%)\n",
            "Epoch: 6\n",
            "Training set:0/70000 (0.0 Loss:0.17613214254379272)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.28533926606178284)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.12730181217193604)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.16005872189998627)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.36480003595352173)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.1765836626291275)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.16436775028705597)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.20668014883995056)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.27401047945022583)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.09418032318353653)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.170463427901268)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.4026314616203308)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2768518924713135)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.19697551429271698)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.3888929486274719)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.29082590341567993)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.17315547168254852)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.18858762085437775)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.19141767919063568)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.43539318442344666)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.15606987476348877)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3303433954715729)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.2662530839443207)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.24211151897907257)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.1266356259584427)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.13772444427013397)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.2768428325653076)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.32029128074645996)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.40233322978019714)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.21793003380298615)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.3090806007385254)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2854851186275482)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.20886105298995972)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2565351724624634)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3754652440547943)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.2741847336292267)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.15827614068984985)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.19208259880542755)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.11153563112020493)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.2530885934829712)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.248773992061615)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.5670807361602783)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.21616517007350922)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.13695456087589264)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.3003433346748352)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.29924845695495605)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.1548711657524109)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.26173537969589233)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.21488884091377258)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2152356654405594)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.22495807707309723)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.20594283938407898)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.29611241817474365)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.23032991588115692)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.3792036175727844)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2665984332561493)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.15624253451824188)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.42320334911346436)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.3351980149745941)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2172224223613739)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.2358855903148651)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.15122902393341064)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.19868113100528717)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.23772811889648438)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.15746290981769562)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.2955757975578308)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.27342766523361206)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.1725279539823532)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.26983779668807983)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.1819487363100052)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.20839816331863403)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.29011619091033936)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.20711976289749146)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.4182758033275604)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.41390037536621094)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2236299067735672)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.24134069681167603)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2735145688056946)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.354743093252182)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2377144694328308)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.25565409660339355)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.36715376377105713)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1750471442937851)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.27747565507888794)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.1618632972240448)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.24973471462726593)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.27683866024017334)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.5036206841468811)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2424262911081314)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3015626072883606)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.16149172186851501)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.1467047780752182)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.1815958172082901)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.21378910541534424)\n",
            "Training set: Average loss: 0.238430\n",
            "Validation set: Average loss: 0.2428874448653619, Accuracy: 9138/10000 (91.38%)\n",
            "Epoch: 7\n",
            "Training set:0/70000 (0.0 Loss:0.20151327550411224)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.32566604018211365)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.28022336959838867)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.2629345655441284)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.1005832850933075)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.2156829833984375)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2247721254825592)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.19665448367595673)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.20157867670059204)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1651058793067932)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.3238777816295624)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2275935560464859)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.342763215303421)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.17822887003421783)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.1401602327823639)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.22788108885288239)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.11642933636903763)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.06851466000080109)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2009747475385666)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.18311507999897003)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.4100511074066162)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.18329860270023346)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.29171478748321533)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.08734089881181717)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.26645782589912415)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.24020028114318848)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.20221401751041412)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1941830813884735)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.1504984050989151)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.13951851427555084)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.22084343433380127)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.20344966650009155)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.21750342845916748)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1466985046863556)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3118124306201935)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.15359698235988617)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.08677011728286743)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.115633524954319)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.08180604875087738)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.31109750270843506)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2975062429904938)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.34434404969215393)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.1967519223690033)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.20776362717151642)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.17577089369297028)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.14053191244602203)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.275486022233963)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.19543319940567017)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2779480516910553)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2022150456905365)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.38707202672958374)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.21493804454803467)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.0950394794344902)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.196950763463974)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.17094501852989197)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2923221290111542)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.21616344153881073)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.3472554385662079)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.09642979502677917)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.1326756328344345)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.22340549528598785)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.09584711492061615)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.4923449456691742)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.25602656602859497)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.22294802963733673)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.22226153314113617)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.1704169064760208)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.23263971507549286)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.19608303904533386)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.1494569331407547)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.33798953890800476)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.1932331621646881)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.29040098190307617)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.15429899096488953)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.252887099981308)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.1390942484140396)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.23637013137340546)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.20955950021743774)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3339693248271942)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1982273906469345)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2051108181476593)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.27124401926994324)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1707552820444107)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.22304905951023102)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.25149714946746826)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.20194745063781738)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.34482553601264954)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.32857364416122437)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2866576313972473)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3982844054698944)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.3003401458263397)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.23051534593105316)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24102886021137238)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2521170973777771)\n",
            "Training set: Average loss: 0.227637\n",
            "Validation set: Average loss: 0.24169855952784894, Accuracy: 9154/10000 (91.53999999999999%)\n",
            "Epoch: 8\n",
            "Training set:0/70000 (0.0 Loss:0.1945963203907013)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.10878151655197144)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.2248021364212036)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.19296053051948547)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.19748780131340027)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.24495454132556915)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.28766700625419617)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.1444936841726303)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.1167839765548706)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.14820390939712524)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.061355412006378174)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.24102170765399933)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2740909159183502)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.2877409756183624)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.2841123938560486)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.1551429033279419)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.25327742099761963)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.1580996960401535)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.21319055557250977)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.33915406465530396)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.17904716730117798)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.4493800103664398)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.11580844223499298)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.14698508381843567)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3141644597053528)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.14508956670761108)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.14468035101890564)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.23286215960979462)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.16495265066623688)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.14831748604774475)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.19465233385562897)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.13501021265983582)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.23871652781963348)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.1241215169429779)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2531205713748932)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.28682318329811096)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.1431233286857605)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2506217062473297)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2541041672229767)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.23157186806201935)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.16086356341838837)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.2337004542350769)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.22782962024211884)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2980944514274597)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.24050372838974)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.32852157950401306)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.07847416400909424)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.2806190550327301)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.22212935984134674)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.1489831954240799)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.07203486561775208)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.19406071305274963)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.19339390099048615)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2751583755016327)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.29527851939201355)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2801530659198761)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.3144485354423523)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.30365926027297974)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2409416139125824)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.22289365530014038)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.08762605488300323)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.25224217772483826)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.23850677907466888)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.20351460576057434)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.20196670293807983)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3666665554046631)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.24284088611602783)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.07360696792602539)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.17907874286174774)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.271565318107605)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.42282363772392273)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.23386666178703308)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.20454229414463043)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20042815804481506)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.20953930914402008)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2733311355113983)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.26024943590164185)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.19911175966262817)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.1615200936794281)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.16218693554401398)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.32157543301582336)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.21646937727928162)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.2584232687950134)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.2182106375694275)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.17424651980400085)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.2613157331943512)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.1856728494167328)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.4690284729003906)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.11646834015846252)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.10990741848945618)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.333914577960968)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.07154619693756104)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.10473213344812393)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.25797393918037415)\n",
            "Training set: Average loss: 0.217334\n",
            "Validation set: Average loss: 0.2780560930823065, Accuracy: 9058/10000 (90.58%)\n",
            "Epoch: 9\n",
            "Training set:0/70000 (0.0 Loss:0.33800679445266724)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.24248436093330383)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.22017017006874084)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.12316372990608215)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2421826422214508)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.1555977165699005)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.19963721930980682)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.19674086570739746)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.17092087864875793)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.20057708024978638)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.34370115399360657)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.14836230874061584)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.20268064737319946)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.3331054449081421)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.26571181416511536)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.23714251816272736)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.2293575406074524)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.10334597527980804)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.1684761494398117)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.34541672468185425)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.19365523755550385)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.18814368546009064)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3828003406524658)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.21586528420448303)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.19591711461544037)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.17539887130260468)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.19165153801441193)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.2849404811859131)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.19859111309051514)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.19394344091415405)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.1782129555940628)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.21204179525375366)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.1770595908164978)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.3343006372451782)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.21801063418388367)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.18738971650600433)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.18751183152198792)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.23688066005706787)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.14331214129924774)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.22761285305023193)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.21901093423366547)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.20897477865219116)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.28388044238090515)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.33239665627479553)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.0889776349067688)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.1047472432255745)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.19217653572559357)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.24836979806423187)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.20452317595481873)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.1577586680650711)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.3596462607383728)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.27548089623451233)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.1971682608127594)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.14541307091712952)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.1811579018831253)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.20816417038440704)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.13603711128234863)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.35408058762550354)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.1825670599937439)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.203191876411438)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.17982855439186096)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.21066056191921234)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.13892239332199097)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.19011186063289642)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.1204545795917511)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.27272626757621765)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.11573202908039093)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.13713589310646057)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.23919755220413208)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.1516016274690628)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.20747533440589905)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.14250509440898895)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3101901113986969)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20671607553958893)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.26621562242507935)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.14113768935203552)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.22133910655975342)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.24083994328975677)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19096393883228302)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1655063033103943)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.32731616497039795)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2194892019033432)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.23463816940784454)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.20637841522693634)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.20601710677146912)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.17066878080368042)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.17599958181381226)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.16675348579883575)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.18134158849716187)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.1408611536026001)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.2268933802843094)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2208171784877777)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.34951499104499817)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.23070144653320312)\n",
            "Training set: Average loss: 0.209800\n",
            "Validation set: Average loss: 0.23038220864714712, Accuracy: 9184/10000 (91.84%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.1702110916376114)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.22475287318229675)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.20337818562984467)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.17489299178123474)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.13508132100105286)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.3492491543292999)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.22256886959075928)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.11202756315469742)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.17675916850566864)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.22828108072280884)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.2437310516834259)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.09783935546875)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.19841107726097107)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.11283712089061737)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.12518607079982758)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.2499532550573349)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.1385464370250702)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.11428332328796387)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.06995853781700134)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.1624380499124527)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.25474444031715393)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.2596592307090759)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.2928098142147064)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.1038774773478508)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.1958029717206955)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.24087996780872345)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.1763298213481903)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.22900450229644775)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.11483794450759888)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.12411599606275558)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.303026407957077)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.31994369626045227)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.05284177511930466)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.21582642197608948)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.1352447271347046)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.2208801507949829)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2640306353569031)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.06457137316465378)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.09058165550231934)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.15634340047836304)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.38421499729156494)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.2487313151359558)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.14182664453983307)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.18645474314689636)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.126188725233078)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.21902132034301758)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.14210084080696106)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.17282381653785706)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.10472941398620605)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.08545669913291931)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.2386253923177719)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2712692618370056)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.21205127239227295)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.1826278269290924)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.08996999263763428)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.16840532422065735)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2609630823135376)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.18223915994167328)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.32582536339759827)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.22048166394233704)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.21031394600868225)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.11794372648000717)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.1574331670999527)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.30806297063827515)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.12398288398981094)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.1671239584684372)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.11308520287275314)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.13949082791805267)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.18668623268604279)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.19374866783618927)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.2821381092071533)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.36930593848228455)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2714378535747528)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.1748182475566864)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.1147683784365654)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.16281536221504211)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.3164553940296173)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.1397504210472107)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.31255799531936646)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17720259726047516)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.1825343519449234)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.18515951931476593)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.21619923412799835)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.17777962982654572)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.22703303396701813)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.214777871966362)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3256194591522217)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.16558937728405)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.17117516696453094)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2924404740333557)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.23694773018360138)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.13564620912075043)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.12797924876213074)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.24308156967163086)\n",
            "Training set: Average loss: 0.201935\n",
            "Validation set: Average loss: 0.24798826208919478, Accuracy: 9156/10000 (91.56%)\n",
            "Accuracy for fold 6: 91.56%\n",
            "Fold 7/7\n",
            "Epoch: 1\n",
            "Training set:0/70000 (0.0 Loss:2.299647808074951)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:1.3457865715026855)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.9247975945472717)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:1.132582426071167)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.8762575387954712)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.7486982941627502)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.8435772061347961)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.5589482188224792)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.5828820466995239)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.5880154967308044)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.4320247173309326)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.3831019699573517)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.5333003997802734)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.33687055110931396)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.6066598892211914)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.35374370217323303)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.3385898172855377)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.4251575469970703)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.36382630467414856)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.5643751621246338)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.43426230549812317)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.4910932779312134)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3970699608325958)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.4660784602165222)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.6063542366027832)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.48971089720726013)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.42682743072509766)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.4992659091949463)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.43105465173721313)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.4340725541114807)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.5295690298080444)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.6108699440956116)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.3355659544467926)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.4495326280593872)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.5643404126167297)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.5643549561500549)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.4852057695388794)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3024604022502899)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.27347636222839355)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.5390511751174927)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.43945908546447754)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3984248638153076)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.38616204261779785)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.486741304397583)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.5717067718505859)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.42040616273880005)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.4974665641784668)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.46989715099334717)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.4730112850666046)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.5678991079330444)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.48562219738960266)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.4409962296485901)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.479309618473053)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2760709524154663)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.32541728019714355)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.39726582169532776)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.6389771699905396)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.4927937090396881)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.7531076669692993)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.4991471469402313)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.37198278307914734)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.4959162473678589)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.4967706501483917)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.39781638979911804)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.27602773904800415)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3460402190685272)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.23691624402999878)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.41619136929512024)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.3246375322341919)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.31668713688850403)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.45681527256965637)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3684426546096802)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.28407493233680725)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.383870929479599)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.3893028497695923)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.523460865020752)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.3761287033557892)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.38079166412353516)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.32269322872161865)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.4614799916744232)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.22614078223705292)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2714058458805084)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.31178417801856995)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.3575945794582367)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.31695041060447693)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.33176857233047485)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3289322555065155)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.26472294330596924)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.21029585599899292)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.3923121988773346)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.26636502146720886)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.5438066720962524)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2937212288379669)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2110360711812973)\n",
            "Training set: Average loss: 0.461621\n",
            "Validation set: Average loss: 0.3400862281964083, Accuracy: 8778/10000 (87.78%)\n",
            "Epoch: 2\n",
            "Training set:0/70000 (0.0 Loss:0.350696861743927)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.34659451246261597)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.4050123989582062)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.4175953269004822)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.27786344289779663)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.18881946802139282)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.317898154258728)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.3441442847251892)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.3987273871898651)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.24421818554401398)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.20013195276260376)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.4330615699291229)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.291110634803772)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.34312883019447327)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.26697584986686707)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.18324729800224304)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.27225032448768616)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.34905630350112915)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.39792099595069885)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2546064257621765)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.31678855419158936)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.38530802726745605)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3869784474372864)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.3835236430168152)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.2250087857246399)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.29223936796188354)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.3860940635204315)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.5615578293800354)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.24426999688148499)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.3292197585105896)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.3770448565483093)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.2971888482570648)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.4526078701019287)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.5217258334159851)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2621420919895172)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.3052765429019928)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3703332841396332)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.44446101784706116)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.4014308750629425)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.32612696290016174)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.36599498987197876)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.36281338334083557)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.37857016921043396)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.6007722616195679)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.24084071815013885)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.20760688185691833)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3710792362689972)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.3495374619960785)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2813267707824707)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.26062846183776855)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.4836399257183075)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.27853092551231384)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.3742045760154724)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.22226464748382568)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.45762142539024353)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.30017656087875366)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.3323327600955963)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.23987659811973572)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.3287731409072876)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.48363223671913147)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3148873746395111)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.25781112909317017)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.3580290377140045)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.28588786721229553)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.39639854431152344)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3432018458843231)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.37913966178894043)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.13372790813446045)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.380820631980896)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.29864200949668884)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.44386810064315796)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.21736913919448853)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.3498731851577759)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.27261853218078613)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.35303664207458496)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.34641265869140625)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.5024682879447937)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.30676785111427307)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.22812487185001373)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.33551156520843506)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.3359382450580597)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.31708794832229614)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.33579587936401367)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.18589511513710022)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.5374141335487366)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.3762648105621338)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3575340807437897)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.4970628619194031)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.3396250009536743)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2403687685728073)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.21940523386001587)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.37460580468177795)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.33760368824005127)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2583863437175751)\n",
            "Training set: Average loss: 0.328220\n",
            "Validation set: Average loss: 0.2961535686330431, Accuracy: 8930/10000 (89.3%)\n",
            "Epoch: 3\n",
            "Training set:0/70000 (0.0 Loss:0.32828137278556824)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.1950550526380539)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.15694069862365723)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.3251199722290039)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.48193761706352234)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.19615474343299866)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.23525097966194153)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.4158145785331726)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.287157267332077)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.32366394996643066)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.26436012983322144)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2514469623565674)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.4352997839450836)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.5002102851867676)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.2921088635921478)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.3216875195503235)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.3486057221889496)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.24508748948574066)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2774512469768524)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.2875540554523468)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.2148805558681488)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.44211170077323914)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.42873498797416687)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.19226092100143433)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.22772060334682465)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.32999056577682495)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.2594938576221466)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.19324523210525513)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.24691317975521088)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.19342941045761108)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.26513439416885376)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.3223456144332886)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.49171924591064453)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.414532870054245)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3687784671783447)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.10773792862892151)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.29109928011894226)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.34586524963378906)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.14905688166618347)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.536388635635376)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.27013686299324036)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.20460763573646545)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.32021209597587585)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.15999123454093933)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.3040042519569397)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.25326457619667053)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.21386867761611938)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.22385714948177338)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.20544013381004333)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.19970625638961792)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.25465908646583557)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.41440820693969727)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.34979355335235596)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.1923225224018097)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.20086905360221863)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2685703933238983)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.22630347311496735)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.34634920954704285)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.39522087574005127)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3541237711906433)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.37471821904182434)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.26380446553230286)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.1236652210354805)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.34947484731674194)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2447759062051773)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.24964499473571777)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3327929377555847)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.23868466913700104)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.29593515396118164)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.3065448999404907)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.46868935227394104)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.17651937901973724)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.24669718742370605)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.19987675547599792)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2629508674144745)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.5065019130706787)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2551548480987549)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2992776036262512)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19026678800582886)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.29458141326904297)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.11687812209129333)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.4576987028121948)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3674390912055969)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.41849565505981445)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.22827214002609253)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.294222891330719)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3155023157596588)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.24582821130752563)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.2888363003730774)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.15613456070423126)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.25820276141166687)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.29332348704338074)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.15296189486980438)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.14329545199871063)\n",
            "Training set: Average loss: 0.289122\n",
            "Validation set: Average loss: 0.2814010849614052, Accuracy: 8937/10000 (89.37%)\n",
            "Epoch: 4\n",
            "Training set:0/70000 (0.0 Loss:0.34267234802246094)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.22221602499485016)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.34775447845458984)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.36173632740974426)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.3521665036678314)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.22069589793682098)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.3694953918457031)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.273280531167984)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.19556589424610138)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3506891131401062)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.33415400981903076)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.21364599466323853)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.2799043655395508)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.15355224907398224)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.34530109167099)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.23348204791545868)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.16694913804531097)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.41817325353622437)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.15208695828914642)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.35464179515838623)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.2559347152709961)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.14912806451320648)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.21250185370445251)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.2853812277317047)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3091019093990326)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.18327495455741882)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.18900606036186218)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.3199366629123688)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.22322769463062286)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.29445314407348633)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.2323073148727417)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.3004269599914551)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.2745709717273712)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.2640828788280487)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.24029560387134552)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.18229666352272034)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2552681863307953)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.3110526502132416)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.27977466583251953)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.19297227263450623)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.26767855882644653)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.23461319506168365)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.17090971767902374)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.20538054406642914)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.23461849987506866)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.23194079101085663)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.20531108975410461)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.19091029465198517)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.1659652143716812)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.30815714597702026)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.30806994438171387)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.2956462502479553)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.34842973947525024)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.35578304529190063)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.22952082753181458)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.41043612360954285)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.23308579623699188)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.24477297067642212)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.2264416515827179)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.20461703836917877)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.30421021580696106)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.3370174467563629)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.3005581796169281)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.45367035269737244)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.2579784691333771)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.24580968916416168)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.2116977572441101)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.20155179500579834)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.16422410309314728)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.15742428600788116)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.3110719323158264)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3557227849960327)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.2043267786502838)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.27011582255363464)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.07257190346717834)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.30074143409729004)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.24830371141433716)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.2232855260372162)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.18976590037345886)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17068102955818176)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.23424440622329712)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.24469858407974243)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.21219998598098755)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.3504222631454468)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.19711196422576904)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.21897606551647186)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.23135176301002502)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.3355455696582794)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.3014458417892456)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.23822154104709625)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.3042084872722626)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2521161735057831)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2325872778892517)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.15554523468017578)\n",
            "Training set: Average loss: 0.266481\n",
            "Validation set: Average loss: 0.2671354432964021, Accuracy: 9023/10000 (90.23%)\n",
            "Epoch: 5\n",
            "Training set:0/70000 (0.0 Loss:0.16750270128250122)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.27753180265426636)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.25896674394607544)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.2282148152589798)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.29799211025238037)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.15602058172225952)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.2149966061115265)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.35447677969932556)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.25606268644332886)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.2035340815782547)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.20693157613277435)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2637084424495697)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.21912725269794464)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.18060414493083954)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.28476959466934204)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.09121078252792358)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.17639407515525818)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.18084047734737396)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.22941575944423676)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.17860156297683716)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.24326229095458984)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.3281817138195038)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3867895305156708)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.36212223768234253)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.23724421858787537)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.17770341038703918)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.23503153026103973)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.18345139920711517)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.2518954575061798)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.320537805557251)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.24156300723552704)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.19026796519756317)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.25583019852638245)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.25030508637428284)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.3777347803115845)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.19146637618541718)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.34404057264328003)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.28247717022895813)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.27839961647987366)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.22769594192504883)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.18537208437919617)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.226350337266922)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.18880845606327057)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2866500914096832)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.1287950873374939)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.20009352266788483)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.26747938990592957)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.1847161203622818)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.3866744041442871)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.33245301246643066)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.15100879967212677)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.19987888634204865)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.2512883245944977)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.07494959235191345)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.11773568391799927)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.1796623170375824)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.36761587858200073)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.30379101634025574)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.290879487991333)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.3676759898662567)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.2083211988210678)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.19074586033821106)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.26402798295021057)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.19010217487812042)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.21097302436828613)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.3007816970348358)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.24368354678153992)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.3476059138774872)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.28989410400390625)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.10365571081638336)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.32616981863975525)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.3774872124195099)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.16113272309303284)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.13275128602981567)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2084508091211319)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.3442056179046631)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2426799237728119)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.39744025468826294)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.19726447761058807)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.2529067099094391)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.32957425713539124)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2832067012786865)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.25762268900871277)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.25086238980293274)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2559051513671875)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.21692030131816864)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.42571428418159485)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.19296656548976898)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.24100585281848907)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.2047721892595291)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.17078398168087006)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.41853809356689453)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.20670770108699799)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2677832543849945)\n",
            "Training set: Average loss: 0.247596\n",
            "Validation set: Average loss: 0.25388709792665615, Accuracy: 9089/10000 (90.89%)\n",
            "Epoch: 6\n",
            "Training set:0/70000 (0.0 Loss:0.2985880672931671)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.2459006905555725)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.34201353788375854)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.14227056503295898)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.2079257369041443)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.21944662928581238)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.21728548407554626)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.4383850693702698)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.15858565270900726)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.30180251598358154)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.16407789289951324)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.2087157666683197)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.28273844718933105)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.434815376996994)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.22260554134845734)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.15393979847431183)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.15789251029491425)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2043198049068451)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.2669603228569031)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.14679232239723206)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.28747260570526123)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.24498315155506134)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.17648860812187195)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.31176185607910156)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.22672852873802185)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.17078320682048798)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.11442460119724274)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.14731065928936005)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.19716529548168182)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.2582736015319824)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.13786610960960388)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.14516422152519226)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.13858269155025482)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.26063844561576843)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.1863621175289154)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.1726682186126709)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2861240804195404)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.2748967111110687)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.17888636887073517)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.30791905522346497)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.2465413361787796)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.23075848817825317)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.2460431456565857)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.18393217027187347)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.2133919894695282)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.24472345411777496)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3020365536212921)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.09872512519359589)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.14060239493846893)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.16455744206905365)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.09688340127468109)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.23288282752037048)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.5362445712089539)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2653771638870239)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.4105018973350525)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.1820305436849594)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.20246315002441406)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.24254699051380157)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.32852113246917725)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.20135115087032318)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.3128478229045868)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.08289634436368942)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.16639500856399536)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2337457835674286)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.23726893961429596)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.299172967672348)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.4302140772342682)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.1280067414045334)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.18661516904830933)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.34413543343544006)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.2974318563938141)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.19298680126667023)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.18811269104480743)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.24935251474380493)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.1332792490720749)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.14750738441944122)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.25568825006484985)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.29791200160980225)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.3533342480659485)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1490010768175125)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2730278968811035)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2235376387834549)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.3147119879722595)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.16639913618564606)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.35477352142333984)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.21827948093414307)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.11864023655653)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.22378431260585785)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.328161358833313)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.21649177372455597)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.2580929398536682)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.1938953995704651)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.1553640067577362)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.336361825466156)\n",
            "Training set: Average loss: 0.233771\n",
            "Validation set: Average loss: 0.268601425988659, Accuracy: 9033/10000 (90.33%)\n",
            "Epoch: 7\n",
            "Training set:0/70000 (0.0 Loss:0.3308076560497284)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.17737294733524323)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.16635002195835114)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.23311610519886017)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.3267664313316345)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.18613336980342865)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.3490598499774933)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.17090147733688354)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.3076985478401184)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.3646572232246399)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.21734832227230072)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.21539923548698425)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.1905234307050705)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.1992153525352478)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.07078917324542999)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.40225279331207275)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.3091821074485779)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.19114336371421814)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.3951079547405243)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.13409112393856049)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.14749178290367126)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.12646040320396423)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.3485445976257324)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.44389283657073975)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.11492563039064407)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.19399100542068481)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.13043779134750366)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.20779076218605042)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.15446946024894714)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.1378227025270462)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.17979781329631805)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.160894975066185)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.1836700290441513)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.16876058280467987)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.27709218859672546)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.29195544123649597)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.27327385544776917)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.12971551716327667)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.2953493893146515)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.18214353919029236)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.14274008572101593)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.14805053174495697)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.0938960611820221)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.4032897353172302)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.24078039824962616)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.10346536338329315)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.2263893336057663)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.14383646845817566)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.2215304672718048)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2224268615245819)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.14492304623126984)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.19708676636219025)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.1946907490491867)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.16727592051029205)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.22259105741977692)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2722774147987366)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.2316727489233017)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.16208165884017944)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.1839822679758072)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.20108206570148468)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.16565074026584625)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.16871626675128937)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.35944950580596924)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2557300925254822)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.22224676609039307)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.28563737869262695)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.3552749752998352)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.10649915039539337)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.47200724482536316)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.07602410018444061)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.3535236120223999)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.17957176268100739)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.20578287541866302)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.11814624071121216)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.2390306293964386)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.5265489220619202)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.2657921314239502)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.15457487106323242)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.28134995698928833)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.24793165922164917)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.18042397499084473)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3029954433441162)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.19957192242145538)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.10550639778375626)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.17376457154750824)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.24893444776535034)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3152134418487549)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.3271314203739166)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.4003055691719055)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.37074705958366394)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.17061308026313782)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.25104835629463196)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2624308168888092)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.29117098450660706)\n",
            "Training set: Average loss: 0.224087\n",
            "Validation set: Average loss: 0.2399633103495191, Accuracy: 9123/10000 (91.23%)\n",
            "Epoch: 8\n",
            "Training set:0/70000 (0.0 Loss:0.13957777619361877)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.17250563204288483)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.16777391731739044)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.09453342854976654)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.21068179607391357)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.13041964173316956)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.21855981647968292)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.16068153083324432)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.3360150456428528)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.34501704573631287)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.10988108813762665)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.13475726544857025)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.14646942913532257)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.19970448315143585)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.30393731594085693)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.19319963455200195)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.17381882667541504)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.19636094570159912)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.14160123467445374)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.19632181525230408)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.1705998331308365)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.18818551301956177)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.30035266280174255)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.24350716173648834)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.37884509563446045)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.24476541578769684)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.24265314638614655)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.1832859069108963)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.19215065240859985)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.2307647168636322)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.23097683489322662)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.15934687852859497)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.19275572896003723)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.17245496809482574)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.2182854861021042)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.2794494032859802)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.3899831175804138)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.11982140690088272)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.1256110817193985)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.1836545318365097)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.10664531588554382)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.19248810410499573)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.18913552165031433)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.15199480950832367)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.147865429520607)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.24408434331417084)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.3052557110786438)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.1965331733226776)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.22318698465824127)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.2028699815273285)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.11867468804121017)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.1593652367591858)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.1236816868185997)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.2082386612892151)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.19984225928783417)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.2101476937532425)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.18916551768779755)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.33247503638267517)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.25332772731781006)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.12373417615890503)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.16749165952205658)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.27354562282562256)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2653173506259918)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.18630942702293396)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.12638036906719208)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.2909490466117859)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.1907999962568283)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.1402992606163025)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.30904942750930786)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.15995177626609802)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.1995222568511963)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.2040151059627533)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.22689691185951233)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.20343117415905)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.17617329955101013)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.15176871418952942)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.12071586400270462)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.21169359982013702)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.1624993085861206)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.1768285036087036)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.37488603591918945)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.21437948942184448)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1557215005159378)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.2965089678764343)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2702922523021698)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.15161381661891937)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.14228101074695587)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.1564931869506836)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.18727953732013702)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.20976045727729797)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.1874026358127594)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.19666025042533875)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.24094225466251373)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.2700032889842987)\n",
            "Training set: Average loss: 0.212161\n",
            "Validation set: Average loss: 0.23376212862266857, Accuracy: 9152/10000 (91.52%)\n",
            "Epoch: 9\n",
            "Training set:0/70000 (0.0 Loss:0.33127546310424805)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.1972140073776245)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.09900642931461334)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.12034299969673157)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.08626522868871689)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.23951521515846252)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.16494792699813843)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.13027063012123108)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.19868211448192596)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.12990954518318176)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.260724276304245)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.10929965227842331)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.085285484790802)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.20726130902767181)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.12410632520914078)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.10176108032464981)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.06357326358556747)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.13921190798282623)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.1420195996761322)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.14624546468257904)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.21069011092185974)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.1596318781375885)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.18980883061885834)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.1752932369709015)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.3876543641090393)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.2101423144340515)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.2109088897705078)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.18330472707748413)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.17472384870052338)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.16960208117961884)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.19347430765628815)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.11573874950408936)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.20294980704784393)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.0991259217262268)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.35940292477607727)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.2629605531692505)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.36553990840911865)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.14071401953697205)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.19679592549800873)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.281566321849823)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.20368090271949768)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.17066679894924164)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.091047003865242)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.2383447140455246)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.21022194623947144)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.18627223372459412)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.2017856240272522)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.22599497437477112)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.14262698590755463)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.13895811140537262)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.1861231029033661)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.44059377908706665)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.18407325446605682)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.18838989734649658)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.14970740675926208)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.26201120018959045)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.23110410571098328)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.2026757150888443)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.32638347148895264)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.1739426553249359)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.12823240458965302)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.10593263059854507)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.19655722379684448)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.2808232605457306)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.062117792665958405)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.14788870513439178)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.22937338054180145)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.22040948271751404)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.14805343747138977)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.282338410615921)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.1875777691602707)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.21702226996421814)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.17797841131687164)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.22282081842422485)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.1293042153120041)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.2330877035856247)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.15697352588176727)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.23563618957996368)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.11685692518949509)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.17462949454784393)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.14813628792762756)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.2106969803571701)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.1483636498451233)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.2869878113269806)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.23684322834014893)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.22144708037376404)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.3724963068962097)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.28720200061798096)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.3379364311695099)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.17895449697971344)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.43231502175331116)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2919166088104248)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.2090500295162201)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.1270398497581482)\n",
            "Training set: Average loss: 0.204045\n",
            "Validation set: Average loss: 0.2415484877149011, Accuracy: 9106/10000 (91.06%)\n",
            "Epoch: 10\n",
            "Training set:0/70000 (0.0 Loss:0.12898992002010345)\n",
            "Training set:640/70000 (1.0660980810234542 Loss:0.20265211164951324)\n",
            "Training set:1280/70000 (2.1321961620469083 Loss:0.09339204430580139)\n",
            "Training set:1920/70000 (3.1982942430703623 Loss:0.24712343513965607)\n",
            "Training set:2560/70000 (4.264392324093817 Loss:0.24786683917045593)\n",
            "Training set:3200/70000 (5.330490405117271 Loss:0.20767229795455933)\n",
            "Training set:3840/70000 (6.3965884861407245 Loss:0.06994687765836716)\n",
            "Training set:4480/70000 (7.462686567164179 Loss:0.22363115847110748)\n",
            "Training set:5120/70000 (8.528784648187633 Loss:0.18572182953357697)\n",
            "Training set:5760/70000 (9.594882729211088 Loss:0.1358816921710968)\n",
            "Training set:6400/70000 (10.660980810234541 Loss:0.19692350924015045)\n",
            "Training set:7040/70000 (11.727078891257996 Loss:0.36148983240127563)\n",
            "Training set:7680/70000 (12.793176972281449 Loss:0.16370539367198944)\n",
            "Training set:8320/70000 (13.859275053304904 Loss:0.27929243445396423)\n",
            "Training set:8960/70000 (14.925373134328359 Loss:0.14853248000144958)\n",
            "Training set:9600/70000 (15.991471215351812 Loss:0.11654230952262878)\n",
            "Training set:10240/70000 (17.057569296375267 Loss:0.15959908068180084)\n",
            "Training set:10880/70000 (18.12366737739872 Loss:0.2406921237707138)\n",
            "Training set:11520/70000 (19.189765458422176 Loss:0.10471116751432419)\n",
            "Training set:12160/70000 (20.255863539445627 Loss:0.12003796547651291)\n",
            "Training set:12800/70000 (21.321961620469082 Loss:0.09728524833917618)\n",
            "Training set:13440/70000 (22.388059701492537 Loss:0.20293545722961426)\n",
            "Training set:14080/70000 (23.454157782515992 Loss:0.19906432926654816)\n",
            "Training set:14720/70000 (24.520255863539447 Loss:0.1718040555715561)\n",
            "Training set:15360/70000 (25.586353944562898 Loss:0.2032976597547531)\n",
            "Training set:16000/70000 (26.652452025586353 Loss:0.15076203644275665)\n",
            "Training set:16640/70000 (27.718550106609808 Loss:0.21061885356903076)\n",
            "Training set:17280/70000 (28.784648187633262 Loss:0.42941099405288696)\n",
            "Training set:17920/70000 (29.850746268656717 Loss:0.16136273741722107)\n",
            "Training set:18560/70000 (30.916844349680172 Loss:0.21243435144424438)\n",
            "Training set:19200/70000 (31.982942430703623 Loss:0.23925156891345978)\n",
            "Training set:19840/70000 (33.04904051172708 Loss:0.18292783200740814)\n",
            "Training set:20480/70000 (34.11513859275053 Loss:0.20935901999473572)\n",
            "Training set:21120/70000 (35.18123667377399 Loss:0.19089187681674957)\n",
            "Training set:21760/70000 (36.24733475479744 Loss:0.16951313614845276)\n",
            "Training set:22400/70000 (37.3134328358209 Loss:0.07239874452352524)\n",
            "Training set:23040/70000 (38.37953091684435 Loss:0.2233281433582306)\n",
            "Training set:23680/70000 (39.44562899786781 Loss:0.10041352361440659)\n",
            "Training set:24320/70000 (40.511727078891255 Loss:0.1723344922065735)\n",
            "Training set:24960/70000 (41.57782515991471 Loss:0.2556725740432739)\n",
            "Training set:25600/70000 (42.643923240938165 Loss:0.0991278812289238)\n",
            "Training set:26240/70000 (43.71002132196162 Loss:0.3067064881324768)\n",
            "Training set:26880/70000 (44.776119402985074 Loss:0.15441511571407318)\n",
            "Training set:27520/70000 (45.84221748400853 Loss:0.0882485881447792)\n",
            "Training set:28160/70000 (46.908315565031984 Loss:0.17970316112041473)\n",
            "Training set:28800/70000 (47.97441364605544 Loss:0.25911059975624084)\n",
            "Training set:29440/70000 (49.04051172707889 Loss:0.14728781580924988)\n",
            "Training set:30080/70000 (50.10660980810235 Loss:0.45210328698158264)\n",
            "Training set:30720/70000 (51.172707889125796 Loss:0.1505688726902008)\n",
            "Training set:31360/70000 (52.23880597014925 Loss:0.12997210025787354)\n",
            "Training set:32000/70000 (53.304904051172706 Loss:0.1984218806028366)\n",
            "Training set:32640/70000 (54.37100213219616 Loss:0.24314948916435242)\n",
            "Training set:33280/70000 (55.437100213219615 Loss:0.29492780566215515)\n",
            "Training set:33920/70000 (56.50319829424307 Loss:0.15222615003585815)\n",
            "Training set:34560/70000 (57.569296375266525 Loss:0.2026718109846115)\n",
            "Training set:35200/70000 (58.63539445628998 Loss:0.15388227999210358)\n",
            "Training set:35840/70000 (59.701492537313435 Loss:0.24859550595283508)\n",
            "Training set:36480/70000 (60.76759061833689 Loss:0.12819339334964752)\n",
            "Training set:37120/70000 (61.833688699360344 Loss:0.08969974517822266)\n",
            "Training set:37760/70000 (62.89978678038379 Loss:0.2579759359359741)\n",
            "Training set:38400/70000 (63.96588486140725 Loss:0.21218352019786835)\n",
            "Training set:39040/70000 (65.0319829424307 Loss:0.20770950615406036)\n",
            "Training set:39680/70000 (66.09808102345416 Loss:0.2610311806201935)\n",
            "Training set:40320/70000 (67.16417910447761 Loss:0.13274088501930237)\n",
            "Training set:40960/70000 (68.23027718550107 Loss:0.20158393681049347)\n",
            "Training set:41600/70000 (69.29637526652452 Loss:0.09592528641223907)\n",
            "Training set:42240/70000 (70.36247334754798 Loss:0.23547443747520447)\n",
            "Training set:42880/70000 (71.42857142857143 Loss:0.17006486654281616)\n",
            "Training set:43520/70000 (72.49466950959489 Loss:0.25846508145332336)\n",
            "Training set:44160/70000 (73.56076759061834 Loss:0.16324731707572937)\n",
            "Training set:44800/70000 (74.6268656716418 Loss:0.30749350786209106)\n",
            "Training set:45440/70000 (75.69296375266525 Loss:0.19865529239177704)\n",
            "Training set:46080/70000 (76.7590618336887 Loss:0.17751479148864746)\n",
            "Training set:46720/70000 (77.82515991471216 Loss:0.3428787589073181)\n",
            "Training set:47360/70000 (78.89125799573561 Loss:0.10381386429071426)\n",
            "Training set:48000/70000 (79.95735607675905 Loss:0.13166318833827972)\n",
            "Training set:48640/70000 (81.02345415778251 Loss:0.1518331915140152)\n",
            "Training set:49280/70000 (82.08955223880596 Loss:0.16816164553165436)\n",
            "Training set:49920/70000 (83.15565031982942 Loss:0.22270305454730988)\n",
            "Training set:50560/70000 (84.22174840085287 Loss:0.15907636284828186)\n",
            "Training set:51200/70000 (85.28784648187633 Loss:0.2333221435546875)\n",
            "Training set:51840/70000 (86.35394456289978 Loss:0.3413920998573303)\n",
            "Training set:52480/70000 (87.42004264392324 Loss:0.19942274689674377)\n",
            "Training set:53120/70000 (88.4861407249467 Loss:0.1311211735010147)\n",
            "Training set:53760/70000 (89.55223880597015 Loss:0.2159782499074936)\n",
            "Training set:54400/70000 (90.6183368869936 Loss:0.24692004919052124)\n",
            "Training set:55040/70000 (91.68443496801706 Loss:0.31643855571746826)\n",
            "Training set:55680/70000 (92.75053304904051 Loss:0.13385167717933655)\n",
            "Training set:56320/70000 (93.81663113006397 Loss:0.10267919301986694)\n",
            "Training set:56960/70000 (94.88272921108742 Loss:0.22004340589046478)\n",
            "Training set:57600/70000 (95.94882729211088 Loss:0.2326144278049469)\n",
            "Training set:58240/70000 (97.01492537313433 Loss:0.2323588877916336)\n",
            "Training set:58880/70000 (98.08102345415779 Loss:0.13575883209705353)\n",
            "Training set:59520/70000 (99.14712153518124 Loss:0.12429730594158173)\n",
            "Training set: Average loss: 0.196855\n",
            "Validation set: Average loss: 0.2255682690413135, Accuracy: 9164/10000 (91.64%)\n",
            "Accuracy for fold 7: 91.64%\n",
            "Average accuracy across all folds: 91.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Initialize lists to store average losses\n",
        "avg_training_loss = []\n",
        "avg_validation_loss = []\n",
        "\n",
        "# Iterate through epochs\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Initialize variables to accumulate losses for the current epoch\n",
        "    epoch_training_losses = []\n",
        "    epoch_validation_losses = []\n",
        "\n",
        "    # Iterate through folds\n",
        "    for fold in range(k):\n",
        "        # Filter losses for the current epoch and fold\n",
        "        epoch_fold_training_losses = [training_loss[i] for i in range(len(epoch_nums)) if epoch_nums[i] == epoch and i % k == fold]\n",
        "        epoch_fold_validation_losses = [validation_loss[i] for i in range(len(epoch_nums)) if epoch_nums[i] == epoch and i % k == fold]\n",
        "\n",
        "        # Aggregate losses for the current fold\n",
        "        epoch_training_losses.extend(epoch_fold_training_losses)\n",
        "        epoch_validation_losses.extend(epoch_fold_validation_losses)\n",
        "\n",
        "    # Calculate average losses for the current epoch and append to lists\n",
        "    avg_training_loss.append(sum(epoch_training_losses) / len(epoch_training_losses))\n",
        "    avg_validation_loss.append(sum(epoch_validation_losses) / len(epoch_validation_losses))\n",
        "\n",
        "# Plot average training and validation loss across all folds per epoch\n",
        "plt.plot(range(1, epochs + 1), avg_training_loss, label='Training')\n",
        "plt.plot(range(1, epochs + 1), avg_validation_loss, label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Average Training and Validation Loss Across Folds per Epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "joCjQ30e9okT",
        "outputId": "fe79217b-3810-4b56-b995-f63d2393f305"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
